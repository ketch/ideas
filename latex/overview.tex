\documentclass[a4paper]{scrartcl}

\usepackage[utf8]{inputenc}
%\usepackage[USenglish]{babel}
\usepackage{csquotes}

%More margins for annotations
%\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage[a4paper, top=3.5cm, bottom=3.5cm, left=3.5cm, right=3.5cm]{geometry}

\usepackage[plainpages=false,pdfpagelabels,hidelinks,unicode]{hyperref}

\usepackage{amsmath}
\usepackage{amsfonts}

\usepackage{graphicx}
\usepackage{subcaption}

% math packages
\usepackage{amsmath}
\numberwithin{equation}{section}
\allowdisplaybreaks
\usepackage{amssymb}
\usepackage{commath}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{nicefrac}

\usepackage{siunitx}

\usepackage{amsthm}
\theoremstyle{plain}
  \newtheorem{theorem}{Theorem}
  \newtheorem{lemma}[theorem]{Lemma}
  \newtheorem{corollary}[theorem]{Corollary}
  \newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
  \newtheorem{definition}[theorem]{Definition}
  \newtheorem{remark}[theorem]{Remark}
  \newtheorem{example}[theorem]{Example}
\numberwithin{theorem}{section}

% definitions
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\CN}{\mathbb{C}}
\renewcommand{\H}{\mathcal{H}}
\renewcommand{\O}{\mathcal{O}}
\newcommand{\dt}{{\Delta t}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\e}{\mathrm{e}}
\newcommand{\scp}[2]{\left\langle{#1,\, #2}\right\rangle}
\newcommand{\I}{\operatorname{I}}
\newcommand{\lot}{\ell_{\mathrm{ot}}}

%tikz
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usetikzlibrary{calc}
\usepackage{color}

% bibliography
\usepackage[%
%   backend=bibtex,bibencoding=ascii,
  backend=biber,
%   style=authoryear-comp, dashed=false,
  style=numeric-comp,
%   firstinits=true, uniquename=init, %abbreviate first names
  giveninits=true, uniquename=init, %abbreviate first names
  natbib=true,
  url=false,
  doi=true,
  isbn=false,
  backref=false,
  maxnames=99,
  ]{biblatex}
\addbibresource{Bib_RK_optim.bib}

\title{Positivity-Preserving Adaptive Runge-Kutta Methods}
\author{Stephan Nüßlein}


\makeatletter
\hypersetup{pdfauthor={\@author}}
\hypersetup{pdftitle={\@title}}
\makeatother

\begin{document}

\maketitle

\section{Introduction}


Many physical processes can be described with differential equations. 
The physical property that are involved in these processes often only make sense if they are positive or non-negative. 
Concentrations or number of particles have to be $\geq 0$.
Other values may only make sense if they comply with other restrictions. An Example of these would-be probabilities or fractions which have to be $\geq 0$ and $\leq 1$.
The ordinary differential equations (ODE) or a partial differential equations (PDEs) are often too complex to solve them analytically and therefore require numerical methods to get an approximate solution.
Numerical methods generally do not satisfy these restrictions on the solutions.

The most common methods to solve ODEs are Runge–Kutta methods (RKMs). 
With the method of lines (MOL) it is also possible to solve PDEs with RKMs.
At first the PDE is discretized in space. The resulting semi-discrete problem is an ODE that can be solved with a RKM. 
It is possible to semi-discretize PDEs in a ways that preserve positivity\cite{kopecz_comparison_2019,kopecz_unconditionally_2018}.

Therefore, it is interesting to know how to use RKMs that preserve positivity.
For RKMs some theoretical limitations are known. One is that any unconditionally positive RKM has an order $\leq 1$. \cite{hundsdorfer_numerical_2003,bolley_conservation_1978,horvath_positivity_1998}. The only well know RKM that is unconditionally positive is the backward euler method, which is obliviously of first order. 
This causes restrictions on the step size for any RKM with a higher order.

There are multiple approaches to get positivity preserving numerical schemes.
One method is the Strong Stability Preserving Runge Kutta (SSPRK). 
The idea is to construct a RKM that can be written as convex combination of euler steps. 
By this the stability properties of the forward euler step are carried over to the SSPRK.
The same happens to the positivity properties. By this the positivity of the method is ensured for time steps $\dt$ with $\dt \leq R \dt_{FE}$, where $R$ depends on the used SSPRK and $\dt_{FE}$ is the forward euler time step \cite{gottlieb_strong_2011}. 
The Modified Patanka Runge Kutta (MPRK) are 
based on the Patanka RKM. In these destruction Terms are scaled by the new solution. This preserves positivity but violates the mass conservation. The MPRK overcome this disadvantage by also scaling the production terms. This is done by solving a linear equation system \cite{kopecz_comparison_2019}.
An approach to ensure positivity in a practical application is shown in \cite{shampine_non-negative_2005}. Ensuring the positivity is done by redefining the ODE outside the domain. Another technique used are event finding methods. This approach is implemented in the Matlab ODE Suite.
Unconditionally positive method of order higher than 1 are the diagonally split Runge-Kutta (DSRK) methods \cite{horvath_positivity_1998}. These are no longer part of the general linear methods. This allows them to be positive and higher order. 

In the following paper an approach is taken that does not overcome the order imitation but is based on the idea that in practical problems for many time steps positivity can be achieved with a higher order methods.
The main idea is to adapt the RKM after computation of the stage values in a way that ensures positivity.  
This means that the computed stage values can be used and don't have to be recomputed. With increasing complexity of the right-hand side, a step reject has a high cost. This makes the additional cost of adapting the RKM economically.
The resulting b should comply with the order conditions as far as possible.

A common way to calculate two different solutions with one set of stage values is the use of embedded methods for error approximation. 
The idea of adapting the weights $b$ after calculating the stage values is used in \cite{ketcheson_spatially_2013}
In this case it is used to adapt the properties of the time integrator for a method of lines simulation on a PDE. With this approach, it is possible to have different properties of the RKM at different parts of the domain. 
Another method that adapts the $b$ are the relaxation Runge–Kutta (RRK) methods. 
In these the weights are scaled by a scalar relaxation parameter to conserve convex quantities. This can be used to conserve the energy or entropy \cite{ranocha_relaxation_2019,ketcheson_relaxation_2019}


In this paper, an adaption of the $b$ is done by solving a Linear programming (LP)-Problem.
These are convex optimization problems where all constraints and the objective function are all affine. These can incorporate both equality and inequality constraints. 
LP-Problems can be quite reliably solved with the Simplex method or with interior point methods. This can be done in reasonable time even for large problems. 
\cite{boyd_convex_2004}

The paper unfolds as follows. In section\,\ref{sec:main_idea} the main idea is explained. How the method can be written as LP-Problem is explained in section\,\ref{sec:LP-Problem}.
In section\,\ref{sec:integration} it is described how the new approach can be used with different RKM, how it can be integrated in a step size control and how the Region of absolute stability can be approximated.  
In section\,\ref{sec:imple} further details that were used when implementing the algorithm are described.
In section\,\ref{sec:Numeric_Results} Numerical results are given for multiple test problems.
A conclusion is given in section\,\ref{sec:conclusion}  

%(TODO: We know that we cannot work around the limitation from \cite{hundsdorfer_numerical_2003}, Where would be a good place to put this?)



\section{Main Idea}\label{sec:main_idea}

When computing the solution of an ODE $u ' = f(t,u) $ using a RKM with $s$ Stages and the Butcher tableau
\begin{align}
\begin{array}{c|c}
c &  A \\
\hline
 & b\\
\end{array}
\end{align}
the stage values $y^{(1)},\cdots,y^{(s)}$ are computed according to
\begin{equation}
y^{(j)} =  u^n + \dt \sum_{k = 1}^{s} a_{jk} f(t^n + \dt c_k,y^{(k)})  \quad j = 1,\cdots,s
\end{equation}
Based on these values the next solution $u^{n+1}$ is computed by
\begin{equation}
u^{n+1} = u^n + \dt \sum_{j  = 1}^s f(t^n + \dt c_j,y^{(j)}) b_j .
\end{equation}
The stage derivatives $f^{(1)},\cdots,f^{(s)}$ can be calculated with
\begin{equation}
f^{(j)} = f(t^n + \dt c_j,y^{(j)})
\end{equation}
The new solution can now be calculated with
\begin{equation}\label{eq:Combination}
u^{n+1} = u^n + \dt F b
\end{equation}
where $F$ is a matrix containing the stage derivatives $f^{(1)},\cdots,f^{(s)}$.

The new solution $u^{n+1}$ does not comply with positivity for some cases. 
So it may be advantageous to adapt \eqref{eq:Combination} in such a way that $u^{n+1}$ is positive, instead of rejecting the Solution and computing a new set of stage values. This is done by adapting the weights $b$.




For the computation of the stage derivatives only the $A$ has to be known. The $b$ is not needed for this computation. 
Based on this Observation we can define $RKM_{A}$ as the set of RKMs that share the same $A$ and only differ in $b$.
Calculating the stage derivatives can be considered as solving the ODE for all RKM in $RKM_A$ at the same time.
The idea now is to choose one $RKM_{(A,b)} \in RKM_{A}$ by fixing the $b$. 
Because the stage derivatives are already known we can choose a $RKM_{(A,b)}$ that leads to a positive $u^{n+1}$, if $RKM_{(A,b)} \in RKM_{A} | u^{n+1} \geq 0$ exists.
When altering the $b$ the properties of an RKM are preserved. 
Especially this approach preserves linear invariants like mass conservation, because it is a regular RKM. 

As it can be see in \eqref{eq:Combination} $u^{n+1}$ is a linear combination of the stage values and the previous solution $u^n$.
Because the coefficients $a_{jl}$ are already fixed the Order Conditions reduce to linear equations.
This means that finding the $b$ can be done by solving a linear Programming problem.
 
If there are no negative values in $u^{n+1}$ with the original $b$, then the method does not cause additional computational effort, because it only has to solve the LP-Problem if the regular RKM does not lead to a positive $u^{n+1}$. %(except for the test)



\subsection{Example I}\label{sec:example_reac}

To illustrate the use of the method we consider the following reaction equation from\,\cite{kopecz_comparison_2019}.

\begin{subequations}
\label{eq:Reaction}
\begin{align}
u_1' &= 0.01u_2 + 0.01 u_3 +0.003u_4 - \frac{u_1 u_2}{0.01+u_1} \\ 
u_2' &= \frac{u_1u_2}{0.01+u_1}-0.01 u_2-0.5(1-\exp(-1.21 u_2^2)) u_3 -0.05 u_2 \\ 
u_3' &= 0.5(1-\exp(-1.21u_2^2)) u_3 - 0.01 u_3 -0.02 u_3 \\ 
u_4' &=0.05 u_2 + 0.02 u_3 0.003u_4 
\end{align}
\end{subequations}
and the initial conditions
\begin{equation}
u(0) = (8,2,1,4)^T
\end{equation}
When solved with the Cash-Karp (CK5) method and $\dt = 0.005$ the approximated solution contains negative values. This causes qualitatively wrong solutions to the problem. 
In Figure\,\ref{fig:exampleI} the obtained results are plotted with a dashed line. 
At $t=1.905$ $u_1$ gets negative. This leads to a diverging solution.

Now the $b$ are adapted. The results are also plotted in Figure\,\ref{fig:exampleI}. 
All $u\geq0$ is now fulfilled. A qualitatively correct solution is obtained.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{plots/exampleI.pdf}
    \caption{Numerical approxiamtion of Reaction Problem from \cite{kopecz_comparison_2019}. Computed with ck5 and $\dt = 0.005$. The dashed line are the reference solution without the adaption of the $b$. The lower plot are the weights $b$ }
    \label{fig:exampleI}
\end{figure}

The used weights $b_1,\cdots,b_5$ are also plotted in Figure\,\ref{fig:exampleI}. 
The used weights are equal the original weights for $t<1.905$. At $t=1.905$ the weights are first adapted to ensure the positivity of the solution. For $t>2.63$ the original set of weights lead to a positive solution. Because of this, the used weights equal the original weights again.


\section{Details (TODO: better title)}\label{sec:LP-Problem}

In the following section, it is explained how the approach can be written as a LP-Problem. 
For this we have to consider which $b$ to choose. 
Generally, any $b \in \R$ can be used. 
A first requirement would be that the new solution is positive.
We can further reduce this space by enforcing the order conditions. 
By incorporating the order conditions, the space of $b$ is limited to a subspace of $\R$. %(Note: not linear because 0 is not in subspace)
Finally, we have to give a objective function to get the optimal $b$.


\subsection{Example II}\label{sec:example_lin}

(Note: The primary goal here is to put the idea of changing the b into perspective. We have shown the general idea and how it works, now we want to discuss on what it does in practice. At first, we state it is good to preserve the order... Get to the Example ... and show that there is more to it)


The main goal is to choose a $RKM_{(A,b)}$ that for which $u^{n+1}$ resembles the solution of the ODE $u(t_{n+1})$. 
A obvious objective is to use a $RKM_{(A,b)}$ with a high order, but this is not enough.
To get a better understanding for the method we consider the behavior of a simple problem.   

We take the linear, positivity preserving ODE from \cite{kopecz_unconditionally_2018} with $u = [u_1,u_2]^T$ with the initial conditions $u_0 = [1,0]^T$ and

\begin{equation}
u'(t) = L u(t) \qquad L = \left[\begin{matrix}- a & 1\\a & -1\end{matrix}\right] \quad a =5
\end{equation}
and the first order RKM with two stages
\begin{align}
\begin{array}{c|cc}
0 &  & \\
\frac{1}{2} & \frac{1}{2} & \\
\hline
 & \frac{1}{2} & \frac{1}{2}\\
\end{array}
\end{align}
If simulated with $\dt = 1$ the stage derivatives are 
\begin{equation}
f^1 = \left(\begin{matrix}-5\\5\end{matrix}\right) \qquad f^2 = \left(\begin{matrix}10\\-10\end{matrix}\right)
\end{equation}
All $b$ that comply with the order Condition for the first order can be expressed as
\begin{equation}
b= \left(\begin{matrix}\frac{1}{2}\\ \frac{1}{2}\end{matrix}\right) + \alpha \left(\begin{matrix}1\\ -1\end{matrix}\right)
\end{equation}
With this we have one degree of freedom for the choice of $b$. This is done by the parameter $\alpha$.
If the general expression for $b$ is inserted in \eqref{eq:Combination} the general solution is 
\begin{equation}
u^{1} = u^0 + \dt  \left[f^1,f^2\right] b = \left(\begin{matrix}\frac{5}{2}\\- \frac{5}{2}\end{matrix}\right) \alpha  \left(\begin{matrix}-15\\15\end{matrix}\right)
\end{equation}
The solution for the original RKM would be $u^{1} = \left(\frac{5}{2} ,- \frac{5}{2}\right)^T$ which contains a negative value. But by changing the parameter $\alpha$ the weights $b$ and the solution is altered.
By a suitable choice of $\alpha \in \left[\frac{1}{6},\frac{7}{30}\right]$ any $u$ that complies with mass conservation and positivity can be reached. 
By adding additional constraints on the $b$ the choice of $\alpha$ can be narrowed down. 
An objective function is also needed to make the choice unique. This should be designed in a way to prefer $b$ that are close to the original weights $b_{orig}$


%This gives raise to the question whether the new solution is reasonable.
%This also poses the problem on developing a system that can decide if the new solution is has to be rejected. 





\subsection{Order Conditions}\label{sec:OrderCond}

The order conditions for a RKM are a set of Equations depending on $a_{jl}$, $c_1,\cdots,c_s$ and $b_1,\cdots,b_s$. In these Equations $b_1,\cdots,b_s$ only appear linearly \cite{hairer_runge-kutta_1993}.
If $a_{jl}$ and $c_1,\cdots,c_s$ are already known, the order conditions for the Order $p$ are an linear equations system and can be written as $Q b = r$. 

The vector $b \in \R^s$ contains the weights and $Q$ is an $g \times s$ matrix, where $g$ is the number of order conditions. The vector $r$ contains the right hand side of the order conditions.

If the $A$ of a preexisting RK-method is used, there is at least one solution for the linear equation system.
In order to get an optimization problem, the equation system has to be underdetermined.
This implies that $\mathrm{rank}(Q) < s$. 
Because the Quadrature conditions are linear independent, there are at least $p$ independent vectors in $Q$. This leads to the condition $p < s$.
This implies that the number of stages has to be higher than the wanted order.



\subsection{Ways to formulate the Optimization Problem}

There are two possible ways to write down the adaptation of the $b$. The first way is to directly adapt the original weights $b_{orig}$ by adding a correction $\Delta b$ that ensures positivity.
This is the easiest way to adapt the $b$. An advantageous property is that the step size can be reduced after calculation the stage values by using the dense output order conditions.  
This approach suits best for explicit methods.
The direct adaption approach is explained in more detail in section\,\ref{sec:direct}.

The second approach is to choose $b$ as a convex combination of a set of $b$s. This approach is more useful if some embedded methods with certain properties are known. 
This approach also shows more predictable behavior because the resulting $u^{n+1}_b$ is also a convex combination of the solutions $u^{n+1}_{b_a},u^{n+1}_{b_b},\cdots$ 
This approach works best with implicit methods.
The direct adaption approach is explained in more detail in section\,\ref{sec:convex}

Both approaches are illustrated in figure\,\ref{fig:b_space}.


\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \input{Image_b_direct}
        \caption{Direct Adaptation}
        \label{fig:b_direct}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \input{Image_b_convex}
        \caption{Convex Combiantion}
        \label{fig:b_convex}
    \end{subfigure}
    \caption{Graphical representation of the two different approaches to adapt the weights}\label{fig:b_space}
\end{figure}

\subsection{Direct Adaptation}\label{sec:direct}

When directly adapting the $b$ a new $b$ that complies with the order conditions is chosen.   
The weights $b = b_{orig} + \Delta b$ consist of the weights of the original method $b_{orig}$ and an adaption $\Delta b$. The adaption is $\Delta b \in \mathrm{ker}(Q)$. 
In Figure\,\ref{fig:b_direct} this is drawn. The red line represents the subspace of $b$ that satisfy the order conditions. Both $b_{orig}$ and $b$ are in this subspace. 
 
To run it as a LP-Problem a set of constraints is constructed.  
Also, an objective function is needed to determine the optimal $b$.

\subsubsection{Constraints}
The Direct Adaptation approach leads to two set of constraints on the $b$.
The first set of constraints is the positivity and can be written as 
\begin{equation}
u_{n+1}=u_n+\dt K \vec{b} \geq 0
\end{equation}
The second set of constraints are the order conditions
\begin{equation}
Qb=r
\end{equation}





\subsubsection{Objective Function}
Different types of objective functions have been tried. 

\begin{description}

\item[L1 norm] The optimization Problem optimizes for $min(|b-rkm.b|_1)$
            
\item[Quadrature]   The optimization Problem optimizes for $min(|b^Tq-r|)$
                            where q is the quadrature condition of the next higher order 
                            and r is the expected right hand side
            
\item[Hom. Order]    The optimization Problem optimizes for $min(|bTO|)$ where O are the homogenous Order Conditions of the next higher orders
                            
 
\end{description}

A reasonable property of the objective function would be that for the LP-Problem only constrained by the order condition the optimal solution is $b = b_{orig}$. 
This can be achieved by $argmin(f_{optim}(b)) = b_{orig}$. 
In practice, this also means that there is no need to solve solve the optimization Problem if no constraints are violated because the solution of this problem is already known. This is advantageous for performance reasons.
The different approaches have been tested on a linear test problem from \cite{kopecz_unconditionally_2018}. The optimization problem was used at every step.
The magnitude of $b$ can get large if the order conditions are used as objective function.
Large values of b can lead to numerical problems. Because $\sum_{i  = 1}^s b_i = 1$, an increase of $||b||$ leads to a combination of negative and positive values. Because the computation is done with a finite precision (and usually in floating point) large $||b||$ can lead to absorption effects.    
By changing the $b$ the stability function is altered. This can lead to oscillations if the poles of the RHS are no longer in the stability region of the resulting method.
For these reasons $||b-b_{orig}||$ seems like a good choice. The 1-Norm can be implement the with Linear Programming using slack variables. We choose 

\begin{equation}
f_{obj} = ||b-b_{orig}||_1
\end{equation}
 as objective function.



\subsubsection{Adaption of Timestep using the dense output formual}

If the original step $\dt$ was too large and there is no suitable $b$, we would like to calculate a $u^{n+1}$ using the stage values instead of rejecting the step. For this the time step is reduced after calculation the stage values. This can be done using the order conditions for dense Output. These can be found in \cite{hairer_runge-kutta_1993}.
We introduce $\theta \in (0,1]$. The new step taken has the length $\theta \dt$ whereas $\dt$ is the time step used when calculating the stage values.
To calculate a reduced step the order conditions have to be adapted and a new $b_{orig}$ for the objective function (and more important the Error detection afterwards) has to be generated. 
(TODO: the $b_{orig}$ is still a open question)

When adapting the Order Condition to a new $\theta$ the Order Condition Matrix $Q$ stays unchanged whereas the right hand side depends on $\theta$.

The dependency of $r(\theta)$ is nonlinear. This means that the maximum $\theta$ with a suitable $b$ cannot be calculated by solving a LP-Problem. This means that different $\theta$ have to be tested. 
(Note: we could include some algorithm to search for it)

\subsection{Convex combination of existing methods}\label{sec:convex}
For some test problems (e.g. the Heat Equation solved with an implicit method of higher order), large distortions occur, even though a norm of the distortion of the b is used as an objective function. 
For implicit methods, it is known that the backward euler steps lead to a positive solution for any $\dt$ \cite{hundsdorfer_numerical_2003}. The backward Euler is still only first Order but shows more exact results than a method of higher Order that is distorted by the adaption of the $b$, at least at some steps. 
A possible method to fix this is to generate the new b based on a convex combination of embedded methods of RKM. 

\subsubsection{Constraints}
At first a set of embedded methods of the RKM $b_1,\cdots,b_d$ are needed. 
If all the used $b$ compile with the order conditions the new method is also of the same Order.
This is straightforward using
$$ Q b = Q (\chi b_{\alpha} + (1-\chi) b_{\beta}) = \chi Q  b_{\alpha} + (1-\chi) Q b_{\beta}) = \chi r + (1-\chi) r = r$$
$b_{\alpha}$ and $b_{\beta}$ are the methods that are combined to $b$ using the parameter $\chi$. The Matrix $Q$ and the vector $r$ are the order conditions as defined in Section\,\ref{sec:OrderCond}.

The new $b$ is defined as 
\begin{equation}\label{eq:b_convex_def}
b = \sum_i^d a_i b_i = Ba
\end{equation}
where $B=\left[b_1,\cdots,b_d \right]$ is a matrix containing the embedded methods.  
To ensure the that \eqref{eq:b_convex_def} yields a convex combination following constraints are enforced for the  the parameters $a,\cdots,a_d$.
\begin{equation}
 0 \leq a_1 \leq 1  \forall {i \in \{1, \cdots d \}}
\end{equation}
\begin{equation}
 \sum_{i=1}^d a_i = 1
\end{equation}
The positivity constraint changes to 
$$u_{n+1}=u_n+\dt K B a \geq 0$$
where $a = [a_1,\cdots,a_d]^T$

\subsubsection{Objective Function}

As an Objective function the function

\begin{equation}
f_{obj} = min \left(\sum_i^d w_i a_i \right)
\end{equation}

is used.  The $w_1,\cdots,w_d$ are weighting factors that determine which is the preferred $b_i$ to use.

Because the optimal solution of the unconstrained problem has to be the original $b$ the weight $w_0$ corresponding to $b_{orig}$ should be the smallest $w$. This weight is simply set to $w_{b_{orig}} = 0$. For the other weights $w_i = \frac{1}{\mathrm{Order} \{b_i\}}$ is used.

\subsubsection{Construction of Embedded Methods}

As a first order method, a $b$ is added that corresponds to a series of backward euler steps. These are known to be yield to a positive result for any step-size.
It is also possible to add embedded methods of higher order.
An important property of the embedded methods is that they should show the desired stability characteristics. 

A Reduction of the step size suing the order conditions for dense output would also be possible. For this a set of embedded methods $b_1(\Theta),\cdots,b_d(\Theta)$ has to be constructed. This was not done in this research.

(Note: Doing dense Output here is a bit complicated, but it would not help ensure positivity only (maybe) increase exactness.  is it worth exploring this or is it better to just leave this open for further research it may be also interesting regardless the study of positivity)
 
\section{TODO: Title} \label{sec:integration}

In the previous sections an algorithm for choosing a $b$ has been presented. 
In the following section, it is explained how this Algorithm can be integrated into the framework of existing solvers.

\subsection{Choice of Baseline Method}
An important property for the used baseline method is the existence of embedded methods and the degrees of freedom for the b.
As noted in Section\,\ref{sec:OrderCond} the number of Stages has to be higher than the order. 
Another important property is the number of degrees of freedom for the choice of the new weights. 
These can be calculated using $s-\mathrm{rank}(Q)$%$\mathrm{dim}(\mathrm{ker}(Q))$.
The number of degrees of freedom for different methods are shown in table\,\ref{table:DOF_exp} for explicit methods and in table\,\ref{table:DOF_imp} for implicit methods.

\begin{table}[h!]
\centering    %Generated below============ 
\begin{tabular}{|l |c c c c c c |} 
 \hline 
Order &1&2&3&4&5&6 \\ 
 \hline Classical RK4&3&2&0&0& - & -  \\ 
 SSPRK(10,4)&9&8&6&4& - & -  \\ 
 Cash-Karp RK5(4)6&5&4&2&1&0& -  \\ 
 Dormand-Prince RK5(4)7&6&5&3&1&0& -  \\ 
 \hline 
 \end{tabular}
 \caption{Degrees of Freedom for Choice of $b$} %Generated above============ 
 \label{table:DOF_exp}
 \end{table}
 
 \begin{table}[h!]
\centering   %Generated below============ 
 \begin{tabular}{|l |c c c c c c |} 
 \hline 
Order &1&2&3&4&5&6 \\ 
 \hline Implicit Euler&0& - & - & - & - & -  \\ 
 Im-Euler 2&2&1& - & - & - & -  \\ 
 Im-Euler 3&5&4&2& - & - & -  \\ 
 Im-Euler 4&9&8&6&3& - & -  \\ 
 \hline 
 \end{tabular}
 \caption{Degrees of Freedom for Choice of $b$} %Generated above============ 
 \label{table:DOF_imp}
 \end{table}


For all methods with the number of stages equal to the order of the method, there are no degrees of freedom without reducing the order. 
If the Classical RK4 method is used the Order has to reduced more because the RK4 method does not have embedded methods of order 3.
Contrary to this methods with $s > p$ have degrees of freedom even without reducing the order. 

Another important property is the stability region in comparison with the maximum $\dt$ that ensures positivity. For this approach to be useful the RK-method has to yield negative results for $\dt$ smaller than the biggest stable $\dt$.

(Note: up to this point I do not understand why this is happening. We only see it occur depending on the Problem-RKM combination)

A third property is the existence of a positive solution. This corresponds to the question if there is an embedded first order method that ensures positivity. This is particularly interesting for Implicit methods, because the $\dt$ is not limited by the stability. 
If we have an embedded first Order Method we can ensure that there is always a positive solution, which might be of first Order. 

\subsection{Error detection and Approximation}
When changing the weights the solution $u^{n+1}$ is changed. 
By incorporating the order conditions for a certain order the space of $b$ that can be choose is limited to a linear Subspace. 
Now it is important to know if the new solution $u^{n+1}$ still resembles the solution of the ODE $u(t_{n+1})$. 
This can be seen in example in section\,\ref{sec:example_lin}.
By a suitable choice of $\alpha \in [\frac{1}{6},\frac{7}{30}]$ any $u$ that complies with mass conservation and positifity can be reached. 

This gives raise to the question whether the new solution is reasonable.
This also poses the problem on developing a system that can decide if the new solution is has to be rejected. 
 
\subsubsection{Error Detection}
For this approach is difficult to give some mathematically rigorous statement regarding the convergence of the method, because it only applies if the error of the original method causes the solution to get negative. This can only happen if the Truncation error is large enough. This requires $\dt \gg 0$ where lower coefficients of the Taylor Series are no longer sufficient to approximate the behavior. 
One could say that the main idea of the adaption of $b$ is to alter the error in a certain way, so that the numerical solution gets 'better' in the sense that it leads to positive solutions. 
The local error of a RKM $e(\dt) =u(t_0 + \dt) - u^1$ can be expressed using the Taylor series of the RKM and the Taylor series of the exact solution. % Hairer I P134

\begin{align}\label{eq:Taylor_sol_ref}
u(t_0 + \dt) &= u(t_0) + u'(t_0) \dt + \frac{u''(t_0)}{2} \dt^2 + \cdots + \frac{u^{(p)}(t_0)}{p!} \dt^p + \frac{u^{(p+1)}(t_0)}{(p+1)!} \dt^{p+1} + \cdots \\
u^{n+1} &= u(t_0)  + v_1 \dt + \frac{v_2}{2} \dt^2 + \cdots + \frac{v_p}{p!} \dt^p + \frac{v_{p+1}}{(p+1)!} \dt^{p+1} + \cdots \\
\end{align}

The derivatives of $u(t)$ can be solely expressed with the partial derivatives of the RHS $f(t,u)$, whereas the coefficients $v_1,v_2,\cdots$ also depend on the RKM. 
In this case the RKM is fixed apart from the weights $b$. 
Therefore $v_1,v_2,\cdots$ are functions of $b$.
Because the $b$ are chosen to ensure the Order Conditions the coefficients $v_1,\cdots,v_p$ are fixed and equal the according derivatives. 
The only remaining degrees of freedom are the coefficients $v_{p+1},v_{p+2},\cdots$.
These can be adapted by changing the $b$.

The main way is to ensure that the RKM with adapted b converges to the correct solution is to ensure that the $b$ approaches $b_{orig}$ for $\dt \to 0$. For this $b$ it is already known that the numeric solution converges to the exact solution.  
This is made sure by property of the Objective function that 
\begin{equation}
\mathrm{argmin}(f_{optim}(b)) = b_{orig}
\end{equation}
This approximation is not very useful for real applications. 
To approximate the error of a new step we propose following approximation of the local error:

\begin{align}
err = |u(t^n)-u^n| &= |u(t^n) - (u^n_{b_{orig}+\dt K(b-b_{orig})})| \\
 &\leq \underbrace{|u(t^n)-u^n_{b_{orig}}|}_{\approx err_T}+\underbrace{|\dt K(b-b_{orig})|}_{= err_{adapt}}
\end{align}

After adapting the $b$ the Approximation Error is checked. If $|\dt K(b-b_{orig})|$ is larger than the tolerance the $b$ is rejected. 

\subsubsection{Steppsize Control}
An important part of an RKM method is the ability to approximate the error of the solution to use it to control the step size. 
The truncation error $|u(t^n)-u^n_{b_{orig}}|$ is usually approximated with an standard error estimator $err_T = | u^{n}_{b_{orig}} - u^{n}_{\hat{b}} |$.

Both errors are added to get an approximation of the total error $err = err_T +w_a err_{adapt}$. The parameter $w_a \leq 1$ is used to account for the fact that the real error is smaller than $err = err_T +err_{adapt}$. The factor also improves the stability of the step size control because $err_{adapt}$ is not as smooth as $err_T$, especially if negative values only occur for a small number of steps.
If the LP-Problem was infeasible and no $b$ was found the $err_{approx}$ is set to a custom value. This value should be chosen sufficiently large to ensure that the next step take is smaller, but still not too large to keep the step size control stable.

This type of error estimation is easy to implement because it can be easily incorporated in an existing step size control and takes advantage of the standard error approximation.

%To make sure that the new adapted solution is still reasonable we measure the deviation from the Original method. In the worst case the errors add up. (todo: here some math formula... (We know that cannot be the case because we already know that we will reduce the error for the quantity we will make positive, still we can use it as a upper bound.))

\subsection{Stability Region}

By adapting the $b$ the used RK Method is changed. This also alters the stability function and therefore changes the region of absolute stability. 
It depends on the solved problem, the RKM and the used $\dt$ on how many steps the stability function is altered. For Problems where only a small number of step are affected a bigger change in the stability function would be acceptable. For Problems with require an adaptation of the weights for every step one needs to make sure that the resulting method is stable.


\begin{figure}
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/stab_ck5.pdf}
         \caption{Cash-Karp 5}
         \label{fig:stab_ck5}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/stab_ex3.pdf}
         \caption{Implicit Extrapolation 3}
         \label{fig:stab_ex3}
     \end{subfigure}
        \caption{Change of stability region for direct adaptation(a) and for convex combination(b) (TODO: legend, axis, description in text)}
        \label{fig:stab}
\end{figure}



\paragraph*{Version 1} $ $\\

The stability function is defined as 
\begin{equation}\label{eq:stab_func}
R(z) = 1 + zb^T(I - zA)^{-1}e
\end{equation}
We can define a function $P(z)$
\begin{equation}
P(z) = R(z)-1 = z b^T (I - zA)^{-1}
\end{equation}
These rational functions are $\in \mathcal{L}^\infty$. (Note: for all explicit methods this is always true, for implicit methods we have to exclude the poles $\{z | \det(I - zA) = 0\}$ or  $\{z | \det(I - zA) > \epsilon \}$ )
We define $\mathcal{P}$ as all $P(z)$ for the class of $RKM_A$. 
This is a vector space.
Proof:
\begin{multline}
\alpha P_a(z) + \gamma P_c(z) =  \alpha z a^T (I - zA)^{-1} + \gamma z c^T (I - zA)^{-1} =\\
z (\alpha a+ \gamma c)^T (I - zA)^{-1} = P_{\alpha a + \gamma c}(z) \in \mathcal{P}
\end{multline}

The $P(z)$ is a linear combination of the rational functions
\begin{subequations}
\begin{align}
p_1(z) &= z e_1^T (I - zA)^{-1}\\
\vdots \\
p_s(z) &= z e_s^T (I - zA)^{-1}
\end{align}
\end{subequations}
whereas $e_1,\cdots,e_s$ is the standard basis of $\R^s$.
These span $\mathcal{P}$. If $p_{e_1}(z),\cdots,p_{e_s}(z)$ are linearly independent $p_{e_1}(z),\cdots,p_{e_s}(z)$ are a basis of $\mathcal{P}$.

With this results the stability function $R_b(z)$ of any RKM in $RKM_{(A)}$ can be calculated with 
\begin{equation}
R_b(z) = 1 + P_b(z) = 1 + \sum_{j = 1}^s b_s p_{e_1}
\end{equation}




\paragraph*{Version 2} $ $\\
The stability function is defined as a function $\CN \to \CN$ with 
\begin{equation}\label{eq:stab_func}
R(z) = 1 + zb^T(I - zA)^{-1}e
\end{equation}
We can add $b$ as a second parameter and get a function $\CN \times \R^s \to \CN$ with
\begin{equation}\label{eq:stab_func}
R(z,b) = 1 + zb^T(I - zA)^{-1}e
\end{equation}
The stability function is a linear function in $b$.

\paragraph*{Version 3} $ $\\
(Ignore this topic, because the following sections also can work without it)

%(Todo: It is not straightforward if there are no pole zero cancellations in the stability function. For explicit methods, the stability function is a polynomial o degree $\leq s$ (Hundsdorfer). Pole zero cancellations cannot happen for these. For implicit functions that are A-stable the poles have to be in the right halve plane. Here is starts to get interesting. Because the A-Stability of one method with a certain $A$ is not a sufficient condition that there are no Poles for all RKM with this A-Matrix. But it is possible (or easy) to prove that if $a_{ii} > 0 \forall i = 1,\cdots , s$ then there are only poles in the right halve plane where they do not matter for the further considerations.)


\subsubsection{Stability of convex combination}
If the new $b$ is chosen as a convex combination of $b$ it is easy to prove properties of the stability region.
The stability function $R_{\chi b_a + (1-\chi) b_b}(z)$ of a convex combination can be calculated by the weighted sum of the Stability functions $R_{b_a}(z)$ and $R_{b_b}(z)$

Proof:
\begin{multline}
R_{\chi b_a + (1-\chi) b_b}(z) = R(z) = 1 + (\chi b_a + (1-\chi) b_b)^Tz(I - zA)^{-1}e \\
= \chi + (1-\chi) + \chi b_a ^Tz(I - zA)^{-1}e + (1-\chi) b_b^Tz(I - zA)^{-1}e \\
= \chi R_{b_a}(z) + (1-\chi) R_{b_b}(z)
\end{multline}

The resulting region of absolute stability is at least the Intersection of the regions of absolute stability of the methods that define the convex combination. This also means that if all the used embedded methods used to construct the new $b$ are A-Stable, the resulting method is also A-Stable.

We want to proof that the convex combination of two methods is absolutely stable at the points where both original methods were absolutely stable.
This can be formulated as $(|R_{b_a}(z)|  < 1) (|R_{b_b}(z)| < 1) \Rightarrow |R_{\chi b_a +(1- \chi) b_b}(z)| < 1$ with $\chi \in [0,1]$.


\begin{multline}
|R_{\chi b_a +(1-\chi) b_b}(z)| = |\chi R_{b_a}(z) + (1-\chi) R_{b_a}(z)| \leq |\chi R_{b_a}(z)| + |(1-\chi) R_{b_a}(z)|\\
 =| \chi| \underbrace{|R_{b_a}(z)|}_{<1} + |(1-\chi)| \underbrace{|R_{b_a}(z)|}_{<1} < \chi + (1-\chi)= 1
\end{multline}



\subsubsection{Stability of  Direct Adaptation}
If the $b$ is adapted directly there is no such proof.
A useful property of the resulting stability region is that it is similar to the stability region of the baseline method. 
(Note: is it possible (or needed at all) to give some statement like: If the stability regions of the RKM used for multiple steps are similar, then the overall method is stable for the intersection of the stability regions.)

For most methods, a small change of $b$ does not alter the stability function dramatically.
(TODO: I am missing a piece here from ‘the derivative is finite’ to ‘is similar’)

We would like to calculate the derivative of the border in respect to a change of $b$. 

The stability function depending on the $b$ is expressed 
When adapting the $b$ we cannot change them independently but only in ways that comply with the order conditions.
$m$ is the number of dimensions of the Kernel of the Order Condition $m = \mathrm{dim}\{\mathrm{Ker} (Q) \}$
The $b_1,\cdots,b_m$  are the Basis vectors of $\mathrm{Ker} (Q)$. With an coordinate Transformation we get the coordinate vector $\hat{b}$ with the Matrix $B = [b_1,\cdots,b_m]$ and $b = B \hat{b}$.

The General Stability function of all embedded RKM that satisfy the Order Conditions $Q$ is given by 

\begin{equation}\label{eq:gen_stabilityf}
R(\hat{b},z) = 1 +  (b_{orig} +B \hat{b})^T z(I - zA)^{-1}
\end{equation}
The border is defined by the implicit function 
\begin{equation}\label{eq:border}
|R(\hat{b},u+iv)|^2 -1 = 0
\end{equation}
This function can be viewed either as $\R^m \times  \CN \rightarrow \R$ or $\R^m \times  \R^2 \rightarrow \R$.
We are only concerned with the expansion or contraction. Because of this we are only interested in the change perpendicular to the border of the stability region. 

For a point $z_0= u_0 +i v_0 $ 
with $ |R(w,z_0)|^2 -1 = 0 $ the gradient of
 $|R(w,u,v)|^2$ is calculated. 
If the gradient does not vanish at $z = z_0$ \eqref{eq:border} Can be transformed in a function $\R^m \times \R \rightarrow \R$ depending on $\hat{b}$ and a single variable $a$ by setting $z = z_0 + n a$. The complex number $n$ denotes the normal vector $\vec{n} = \frac{\nabla |R(\hat{b},u,v))|^2}{\left| \nabla |R(\hat{b},u,v))|^2 \right|}$ of the border of the stability region written in complex notation. This defines the function $a(\hat{b})$ with $|R(\hat{b},z_0 + n a(\hat{b}))|^2 -1 = 0$. 
This implies that this approach is only applicable if the gradient does not vanish on the border of the stability function. This is not true in general but for the most common RKM. 
The derivative $\frac{\mathrm d}{\mathrm d \hat{b}} (a(\hat{b}))$ can be calculated using the implicit function theorem. The defining function is 
\begin{equation}\label{eq:f(w,a)}
f: \R^m \times \R \rightarrow \R f(\hat{b},a) = |R(\hat{b},z_0 + n a)|^2 -1 
\end{equation}
If explicit methods are used this function is known to be continuously differentiable because it can be written as a Polynomial of $\hat{b},u$ and $v$. This is a necessary condition for the use of the of the Implicit function theorem. 
It is also known by definition, that for the point $b = b_{orig}$ and $z = z_0$ the equation $ |R(\hat{b},z_0)|^2 -1 = 0 $ is satisfied.
For simplicity only the formulas for the derivative at the value $b = b_{orig}$ are given. This corresponds to $\hat{b} = 0$.
The wanted derivative is given by 

\begin{equation}
 \frac{\partial a}{\partial \hat{b}_j} (w) =
 - \left[ \frac{\partial f}{\partial a} (\hat{b},a(\hat{b}))  \right] ^{-1} 
   \left[ \frac{\partial f}{\partial \hat{b}_j}(\hat{b},a(\hat{b})) \right]
\end{equation}
as long as $\frac{\partial f}{\partial a} \neq 0$ on the border of the region of absolute stability. This is true in most cases as it has been discussed above.

The Derivative of \eqref{eq:f(w,a)} in respect to $a$ evaluated at $\hat{b}=0$ can be calculated using the directional derivative
\begin{align*}
 \frac{\partial f}{\partial a} \Big|_{\hat{b}=0} =& 
 \frac{\partial f}{\partial a} (\hat{b},a(\hat{b})) \Big|_{\hat{b}=0} = 
 \nabla |R(\hat{b},a(\hat{b})|^2 \vec{n} \Big|_{\hat{b}=0} \\
=& \left| \nabla|R(0,z_0)|^2 \right| = \left| \nabla|R_{b_{orig}}(z_0)|^2 \right|
\end{align*} 

The derivative in respect to $\hat{b}_j$ is 
$ \frac{\partial f}{\partial \hat{b}_j}(\hat{b},a(\hat{b})) \Big|_{\hat{b}=0}$
this simplifies to 
\begin{multline}\label{eq:derivative_to_b}
 \frac{\partial f}{\partial \hat{b}_j}(\hat{b},a(\hat{b})) \Big|_{\hat{b}=0} = 
 \frac{\partial }{\partial \hat{b}}(R(\hat{b},z)R(\hat{b},z^*)) \\
 = (B\hat{b}_j)^T (I-zA)^{-1} R_{b_{orig}}(z^*) + (B\hat{b}_j)^T (I-z^*A)^{-1} R_{b_{orig}}(z)
\end{multline}
using the fact that $z_0$ is on the border of the stability region.
The derivative $(B\hat{b})^T (I-zA)^{-1} R_{b_{orig}}(z^*) (B\hat{b})^T (I-z^*A)^{-1} R_{b_{orig}}(z)$ is known to be finite for explicit methods, because it is a polynomial and $|z_0| < \infty$
The same process can be done for all points on the border of the stability region. 
As long as $\nabla|R(\hat{b},u+iv)|^2 \neq 0 \forall \hat{b} < \epsilon  \forall z$ the derivative $\frac{\partial a}{\partial \hat{b}_j} (\hat{b})$ is finite. This means that a small change the $b$ only leads to a small change of the stability region.


\section{Implementation aspects of the Algorithm}\label{sec:imple}

TODO: Discuss in this section aspects that are needed for implementation (and improve the usability) but do not concern the mathematical idea of the approach.

\subsection{Positifity}


For the further discussion, we are concerned with a System of $m$ coupled ODEs. 
In this setup $u \in \R^m$.
Systems of ODEs with a high number of dimensions can appear when solving a PDE with the method of lines.

To enshure that the solution is positive 

\begin{align}
 u_i^{n+1} &\geq 0   \;   \forall {i \in \{1, \cdots,m \}}  \\
 u_i^n + h \sum_{j=0}^{s-1} b_j f_i^j  &\geq 0   \;   \forall {i \in \{1,\cdots,m \}}  
\end{align}
has to be fulfilled.
This infers $m$ positivity constraints to the optimization problem. These can be written as

\begin{equation}
u_i + \dt F  b \geq 0     
\end{equation}
where $F = \big[f^1 , \cdots f^{s-1}\big]$.

This causes unnecessary many inequality constraints on the LP-Problem. To simplify the problem, one want to reduce the number of positivity constraints by ignoring it for ODEs where there is no issue with positivity.
The new positivity constraints can be formulated as
\begin{equation}
u_i^n + \Delta b \sum_{j=0}^{s-1} b_j f_i^j  \geq 0   \;   \forall {i \in h \subseteq \{1,\cdot,m \}} 
\end{equation}
For this a set $h \subseteq \{1,\cdots,m \}$ that include all indecies that are needed for determining the $b$ has to be chosen.

A reasonable approach is to set $h_0 = \{ i \in \{1,\cdots,m \} |  u_i^{n+1}  < 0 \}$. 
After the new $b$ and $u^{n+1}$ is computed using $h_0$, the algorithm has to check if the positivity of the other values $u_i | i \notin h_0 \subseteq \{1,\cdots,m \}$ is still meet. 
If the conditions is not meet a new set $h_{a+1} = \{ i \in \{1,\cdots,m \}|  u_i^{n+1}  < 0 \} \cup h_{a}$ using the new $u^{n+1}$ is generated and the LP-Problem is solved again. This process id repeated until a solution is found that leads to a $u^{n+1} \geq 0$. The choice of $h_{a+1}$ based on the $h_{a}$ ensures that the algorithm cannot get stuck in a loop. In the worst case $h_a$ is growing very slowly until $h_a = \{1,\cdots,m \}$. 

This effect did not occur so far. 
(Note: A approach on this would be to make a educated guess which indices could get a problem with positivity. A possible estimate would be $\frac{u_i^{n+1}}{max(K_{(i,0)}, \cdots ,K_{(i,0)})} $ )

When enforcing a maximum value the number of constraints can be reduced using the same technique. When both enforcing maximum and minimum values two separate sets of active constraints $k_{min}$ and $k_{max}$ are used. The set $k_{max}$ denotes the active maximum constraints and the set $k_{min}$ denotes the active minimum constraints. 
Here it is important to update these sets simultaneously.  

\subsection{•}
(Todo: Maybe a flowchart and a description on how we stitch everything together.) 

The algorithm on adapting the b consists of two main loops. The outer loop loops through all the $\theta$, staring with $\theta = 1$ down to the lowest $\theta$. The inner loop loops through the orders, starting with the highest Order down to the lowest Order. 
Until an acceptable $b$ the algorithm tests combinations of $\theta$ and $p$. At first the Order Conditions are constructed. On these a LP-Solver is invoiced. If the LP-Problem is infeasible the algorithm immediately tries the next combination. If the LP-Problem is feasible we test if the $b$ is acceptable. At first it is tested weather the new $u^{n+1}$ indeed complies with the constraints. This is necessary because sometimes the used LP-solvers incorrectly identify the problem as feasible. (Note: because of the chosen objective function it cannot be unbounded) As second test the deviation from the old solution (Compare Error estimation) is calculated. If the error is bigger than a set maximum error the $b$ is also rejected and the algorithm also switch to the next combination.
As soon as the algorithm finds a suitable $b$ the loops are aborted and the $b$ is used for the update step.
If all the combinations of $\theta$ and $p$ did not lead to an acceptable $b$ the step has to be rejected and a new set of stage values has to be computed.

(Note: It would also be possible to use another strategy for traversing all the combinations of $(p,\theta)$. Is it worth exploring these? Eg, a bisect algorithm for $\Theta$ or linear approximations)


\section{Results of experiments}\label{sec:Numeric_Results}

(Note: Is here a good place to give a note on where supplementary code can be found?)

(Note: Should we discuss which solver is used. Probably it is not worth using space for some plot etc. on this... Maybe a short note?)

\subsection{Applicable Problems}\label{sec:app_problem}
The approach can be used with ODE systems $u' = f(t,u)$ that ensure  $u(t) \geq 0 \forall t \forall {  u_0 \geq 0}$ 
A sufficient condition is  
\begin{equation}
u_i=0 \Rightarrow f_i(t,u_1,\cdots,u_i,\cdots,u_n) \geq 0  \forall {u_c \geq 0} \forall {t}
\end{equation}
For problems where the exact solution is positive for certain $u_0$ but positivity is not preserved for all $u_0 \geq 0$ tests did not show promising results. Additional it is not certain if the computed solutions would be reasonable.


\subsection{Explicit methods}
For most of the test problems problems with stability occur before getting negative values. This means that we need methods with large stability region.



\subsection{Implicit methods}
Implicit methods seem like an advantageous choice for a couple of reasons.

When tested on the diffusion equation with an spike as initial data the $b$ is only changed for the first step. The change of the $b$ can lead to reasonable changes of the hight of the maximum. 

\subsection{Stepsize control}

Reaction Problem with step-size control

\subsection{Dense Output}


\section{Conclusion} \label{sec:conclusion}




\printbibliography



\end{document}