\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
%\usepackage[USenglish]{babel}
\usepackage{csquotes}

%More margins for annotations
%\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage[a4paper, top=3.5cm, bottom=3.5cm, left=3.5cm, right=3.5cm]{geometry}

\usepackage{amsmath}
\usepackage{amsfonts}

\usepackage{graphicx}
\usepackage{subcaption}

% math packages
\usepackage{amsmath}
\numberwithin{equation}{section}
\allowdisplaybreaks
\usepackage{amssymb}
\usepackage{commath}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{nicefrac}

\usepackage{siunitx}

\usepackage{amsthm}
\theoremstyle{plain}
  \newtheorem{theorem}{Theorem}
  \newtheorem{lemma}[theorem]{Lemma}
  \newtheorem{corollary}[theorem]{Corollary}
  \newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
  \newtheorem{definition}[theorem]{Definition}
  \newtheorem{remark}[theorem]{Remark}
  \newtheorem{example}[theorem]{Example}
\numberwithin{theorem}{section}

\usepackage[%
  backend=bibtex,bibencoding=ascii,
%   backend=biber,
%   style=authoryear-comp, dashed=false,
  style=numeric-comp,
%   firstinits=true, uniquename=init, %abbreviate first names
  giveninits=true, uniquename=init, %abbreviate first names
  natbib=true,
  url=true,
  doi=true,
  isbn=false,
  backref=false,
  maxnames=99,
  ]{biblatex}
\addbibresource{Bib_RK_optim.bib}

\usepackage{booktabs}
\usepackage{multirow}

% load hyperref after amsmath to get rid of stupid ``destination with the same identifier...'' warnings
\usepackage[plainpages=false,pdfpagelabels,hidelinks,unicode]{hyperref}

% suppress "multiple pdfs with page group included in a single page"
% http://tex.stackexchange.com/questions/198586/conditional-based-on-the-version-of-pdflatex
\begingroup\expandafter\expandafter\expandafter\endgroup
\expandafter\ifx\csname pdfsuppresswarningpagegroup\endcsname\relax
\else
  \pdfsuppresswarningpagegroup=1\relax
\fi

% definitions
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\CN}{\mathbb{C}}
\renewcommand{\H}{\mathcal{H}}
\renewcommand{\O}{\mathcal{O}}
\newcommand{\dt}{{\Delta t}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\e}{\mathrm{e}}
\newcommand{\scp}[2]{\left\langle{#1,\, #2}\right\rangle}
\newcommand{\I}{\operatorname{I}}
\newcommand{\lot}{\ell_{\mathrm{ot}}}
\newcommand{\ut}{\tilde{u}}
\newcommand{\bt}{\tilde{b}}
\newcommand{\pt}{{\tilde{p}}}
\newcommand{\perturb}{perturb}
\newcommand{\todo}[1]{{\Large{\color{red}{#1}}}}

%pseudocode
\usepackage[boxed]{algorithm}
\usepackage{algpseudocode}


%tikz
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usetikzlibrary{calc}
\usepackage{color}

% LP-Box
\newcommand{\LPblock}[1]{
\tikzstyle{textbox} = [draw=black, fill=white, very thick,
    rectangle, rounded corners, inner sep=10pt, inner ysep=20pt]
\tikzstyle{titlebox} =[fill=white,draw=black, text=black]
\begin{center}
\begin{tikzpicture}
\node [textbox] (box){%
\begin{minipage}{0.8\textwidth}
\vspace{-0.3cm}
{#1}
\vspace{-0.3cm}
\end{minipage}
};
\node[titlebox, right=10pt] at (box.north west) {LP};
\end{tikzpicture}
\end{center}
}


\title{Positivity-Preserving Adaptive Runge--Kutta Methods}
\author{Stephan Nüßlein \and Hendrik Ranocha \and David I. Ketcheson}


\makeatletter
\makeatother

\begin{document}

\maketitle

\begin{abstract}
%(i) General motivation
Many important differential equations model quantities whose value
must remain positive or stay in some bounded interval.
%(ii) Specific problem
These bounds may not be preserved when the model is solved numerically.
%(iii) how you propose to solve the problem
We propose to ensure positivity or other bounds by applying Runge--Kutta
integration in which the method weights are adapted in order to
enforce the bounds.  The weights are chosen at each step after calculating the
stage derivatives, in a way that also preserves (when possible) the order of
accuracy of the method.  The choice of weights is given by the solution
of a linear program.
%(iv) how you verified your idea and  important results and possible/important limitations
We investigate different approaches to choosing the weights by considering
adding further constraints.
We also provide some analysis of the properties
of Runge--Kutta methods with perturbed weights.  Numerical examples demonstrate
the effectiveness of the approach, including application to both explicit and
implicit methods.
\end{abstract}



\section{Introduction}


Many physical processes can be described with differential equations.
The physical quantities that are involved in these processes often only make sense if they remain within certain bounds.
For instance, concentrations must be non-negative (we will often say simply {\em positive} for short), while
probabilities or mass fractions must remain in $[0,1]$.
The ordinary differential equations (ODEs) or partial differential equations
(PDEs) that model these quantities are often too complex to be solved
analytically and therefore require numerical approximation.
Numerical methods generally may not satisfy these bound constraints.
In the present work, we develop an approach to ensuring positivity
or other bound constraints using Runge--Kutta methods (RKMs) for the
solution of ODEs or semi-discretized PDEs.

We say an ODE
\begin{align} \label{ode}
    u'(t) & = f(t,u)
\end{align}
in $\R^m$ is positive if
\begin{align} \label{continuous-positivity}
    u(0)\ge 0 \implies u(t) \ge 0 \text{ for all } t>0.
\end{align}
A sufficient condition for positivity of an ODE is
\begin{equation} \label{eq:condition_ODE_pos}
u_i=0 \implies f_i(t,u_1,\cdots,u_i,\cdots,u_n) \geq 0 \quad \forall {u_c \geq 0}\; \forall {t}
\end{equation}
For such ODEs a step using the forward Euler method will lead to a positive result for small $\dt$.
A step using the backward Euler method will lead to a positive result for any $\dt$.
Any RKM (or in fact any general linear method) that is unconditionally positivity preserving for all positive ODEs
must have order $\le 1$ \cite{bolley_conservation_1978}. Notably, the (first-order)
backward Euler method is unconditionally positivity preserving.
For any higher order method, we expect positivity only under some restriction
on the time step size.

Several approaches to ensuring numerical positivity exist in the literature.
One technique is to use event finding methods in order to stop when some quantity
reaches zero and then proceed in some special way. This approach is implemented in the MATLAB ODE Suite.
Another approach to ensure positivity in a practical application is shown in
\cite{shampine_non-negative_2005}, where the ODE is redefined outside the
physical domain.
If positivity is preserved under a forward Euler step (with
some step size restriction $\dt \le \dt_\text{FE}$), then any strong stability preserving Runge--Kutta (SSPRK)
method will also preserve positivity (with a modified step size restriction)~\cite{gottlieb_strong_2011}.
Specifically, the positivity of the method is ensured for time steps
$\dt \leq {\mathcal C} \dt_\text{FE}$, where ${\mathcal C}$ depends on the SSPRK method.
Modified Patankar--Runge--Kutta (MPRK) methods represent another approach to ensuring
positivity for specific classes of ODEs. MPRK methods introduce multiplicative
factors within the Runge--Kutta stages to ensure positivity, but require the solution
of a linear algebraic system; see e.g.\ \cite{kopecz_comparison_2019} and references therein.
Finally, we mention diagonally split Runge--Kutta (DSRK) methods, which can be unconditionally
positive and have order higher than one. As MPRK schemes, DSRK methods avoid
the restriction mentioned above because
they are not general linear methods \cite{horvath_positivity_1998}.  However, in practice DSRK
methods are less accurate than backward Euler for large step sizes \cite{macdonald2007}.

The rather discouraging theoretical result of \cite{bolley_conservation_1978} shows that one
should not hope to preserve positivity with a single method for every problem and every
initial condition.  In the present work we take an approach based on the idea that for
a particular problem and initial condition, there often exists a method of high order
that is positivity preserving, at least for a single step.
The main idea is to adaptively choose the weights $b$ of the RKM, after
the stage values are known, in a way that ensures positivity.
The selection of the weights requires the solution of a linear program (LP) at every
step for which the numerical solution would otherwise be non-positive.
This is a significant cost, but may in fact be an economical alternative to rejecting
a step or using excessively small step sizes.

The idea of using different weights within an RKM is not new; for instance it is
the basis of error approximation using embedded RK pairs \cite{hairer_solving_1993}.
The idea of adapting the weights after calculating the stage values has also been used,
for instance in \cite{ketcheson_spatially_2013}.
In this case it is used to adapt the properties of the time integrator for a method of lines solution of a PDE. With this approach, it is possible to have different properties of the RKM at different parts of the domain.
Another class of methods that adapt the weights at the end of an RK step
are the relaxation Runge--Kutta (RRK) methods.
In these, the weights are scaled by a scalar relaxation parameter in order to guarantee
conservation or monotonicity of a desired functional; e.g. to conserve or
dissipate energy or entropy
\cite{ketcheson_relaxation_2019,ranocha_relaxation_2019,ranocha2020general}.

Our means to ensure positivity can be interpreted as a projection
approach, where the numerical solution is adapted to satisfy the
positivity constraint at the end of each step. In contrast to the
usual orthogonal projection, which has also been proposed to deal
with positivity constraints \cite{shampine1986conservation},
our approach preserves all linear invariants of the given ODE. These
invariants can be very important. e.g.\ the total mass for a transport
problem or in reaction systems. The property to preserve all linear
invariants is shared by RRK methods and has been shown to be an
important advantage compared to orthogonal projection methods
\cite{ranocha2020relaxationHamiltonian}. Of course, it is also
possible to enforce the preservation of linear invariants in projection
methods, but these invariants have to be known explicitly
\cite{sandu2001positive}.

The paper unfolds as follows. In Section~\ref{sec:main_idea} the main idea is explained. How the method can be written as an LP is explained in Section~\ref{sec:LP}.
Section~\ref{sec:integration} describes how the new approach can be used with different RKMs, how it can be integrated in a step size control, and how the region of absolute stability can be approximated.
In Section~\ref{sec:imple} further details that were used when implementing the algorithm are described.
In Section~\ref{sec:Numeric_Results} numerical results are given for multiple test problems.
A conclusion is given in Section~\ref{sec:conclusion}.

%(TODO: We know that we cannot work around the limitation from \cite{hundsdorfer_numerical_2003}, Where would be a good place to put this?)


\section{Bound-preserving adaptive Runge--Kutta methods}\label{sec:main_idea}

When computing the solution of an ODE $u ' = f(t,u) $ using an RKM with $s$ stages and the Butcher tableau
\begin{align}
\renewcommand{\arraystretch}{1.2}
\begin{array}{c|c}
c &  A \\
\hline
 & b^T\\
\end{array}
\end{align}
the stage values $y_1,\cdots,y_s$ are computed according to
\begin{equation}\label{eq:stagevalues}
y_j =  u^n + \dt \sum_{k = 1}^{s} a_{jk} f(t^n + \dt c_k,y_k),  \quad j = 1,\cdots,s.
\end{equation}
Based on these values, the next solution $u^{n+1}$ is computed as
\begin{equation} \label{eq:rkstep}
u^{n+1} = u^n + \dt \sum_{j  = 1}^s f(t^n + \dt c_j,y_j) b_j .
\end{equation}
Let $f_j = f(t^n + \dt c_j,y_j)$; then we can write \eqref{eq:rkstep} as
\begin{equation}\label{eq:Combination}
u^{n+1} = u^n + \dt F b,
\end{equation}
where the $j$th column of $F$ is $f_j$.
We wish to impose the discrete analog of \eqref{continuous-positivity}; i.e.
\begin{align} \label{positivity}
    u^n\ge 0 \implies u^{n+1} \ge 0,
\end{align}
or more general bound constraints
\begin{align}
    \alpha \le u^n\le \beta \implies \alpha \le u^{n+1} \le \beta.
\end{align}
We will focus on the case of positivity while keeping in mind that
the methodology extends to general bounds.
The main idea of the present work is that if the new solution $u^{n+1}$ contains
negative entries, we can replace the weights in \eqref{eq:Combination} with
a set of modified weights $\bt$ such that the resulting solution is positive:
\begin{equation}\label{eq:ut}
\ut^{n+1} = u^n + \dt F \bt^n \ge 0.
\end{equation}
Indeed, we can view \eqref{eq:ut} as a linear constraint on the choice of
modified weights $\bt^n$.  Since we have already computed the intermediate stages,
$F$ is a known, fixed matrix.  In order to ensure that the modified solution $\ut^{n+1}$
is accurate, we can also constrain $\bt$ to satisfy the Runge--Kutta order conditions
up to some order (ideally, the same order as the original method).  Observe that
all of the order conditions are linear in the weights, so that these additional constraints
take the form
$$
Qb = r
$$
for some fixed matrix $Q$ and vector $r$.
By applying this technique at each step, we integrate \eqref{ode} with a
sequence of Runge--Kutta methods with coefficients $(A,\bt^n)$.  At any step
for which the solution $u^{n+1}$ produced by method $(A,b)$ is positive, we do not
need to modify the weights and can simply accept this unmodified solution.
Note that linear invariants (such as mass conservation) of the solution are
automatically preserved in this approach, since at each step we use a Runge--Kutta
method.

We see that the choice of $\bt$ is subject to linear equality and inequality
constraints.  If we choose a linear objective function, the resulting problem
for finding the modified weights is a linear program, which can be efficiently
solved by standard algorithms.  A natural choice of objective function is
$$
\text{minimize } \|\bt - b\|_1.
$$
The resulting problem can be phrased as an LP by using slack variables.
In general, this LP may not have a solution; we can relax the constraints
by requiring a lower order of consistency than the design order of the
method.  These choices and alternatives will be considered in Section~\ref{sec:LP}.

In contrast to other projection methods
\cite{shampine1986conservation,sandu2001positive},
minimizing the deviation of the weights instead of the deviation
of the projected solution is computationally much more efficient
for large systems, arising for example in the discretization of
PDEs.


\subsection{Example I}\label{sec:example_reac}

To illustrate the usage of the method we consider the reaction system
\cite{kopecz_comparison_2019}
\begin{subequations}
\label{eq:Reaction}
\begin{align}
u_1' &= 0.01u_2 + 0.01 u_3 +0.003u_4 - \frac{u_1 u_2}{0.01+u_1}, \\
u_2' &= \frac{u_1u_2}{0.01+u_1}-0.01 u_2-0.5(1-\exp(-1.21 u_2^2)) u_3 -0.05 u_2, \\
u_3' &= 0.5(1-\exp(-1.21u_2^2)) u_3 - 0.01 u_3 -0.02 u_3, \\
u_4' &=0.05 u_2 + 0.02 u_3 + 0.003u_4,
\end{align}
\end{subequations}
with initial conditions
\begin{equation}
u(0) = (8,2,1,4)^T.
\end{equation}
When solved with the Cash-Karp (CK5) \cite{cash1990variable} method and $\dt = 0.005$ the approximated solution contains negative values. This causes qualitatively wrong solutions to the problem.
In Figure\,\ref{fig:exampleI} the obtained results are plotted with dashed lines.
At $t=1.905$ the value of $u_1$ gets negative. This leads to a diverging solution.

Now the weights are adapted. The adapted weights are of 4th order. The results are also plotted in Figure\,\ref{fig:exampleI}, with solid lines.
The positivity constraint is now fulfilled. A qualitatively correct solution is obtained.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\textwidth]{plots/exampleI.pdf}
    \caption{Numerical approximation of the reaction problem \eqref{eq:Reaction} computed with CK5 and $\dt = 0.005$. The dashed lines are the approximations obtained without the adaption of the weights. The lower plot shows the adaptation of the weights. }
    \label{fig:exampleI}
\end{figure}

The used weights are also plotted in Figure\,\ref{fig:exampleI}.
The used weights are equal to the original weights for $t<1.905$. At $t=1.905$ the weights are first adapted to ensure the positivity of the solution. For $t>2.63$ the original set of weights lead to a positive solution. Because of this, the used weights equal the original weights again.


\section{Selection of modified weights}\label{sec:LP}

In this section, we consider the formulation of the LP to choose
the modified weights $\bt$.  In particular, we focus on the choice
of objective function and how to relax the constraints to ensure that
a feasible solution exists.

\subsection{Example II}\label{sec:example_lin}

The main goal is to choose a method $(A,b)$ such that $u^{n+1}$
approximates the solution of the ODE $u(t_{n+1})$.
An obvious objective while modifying the Runge--Kutta coefficients
is to retain a high order of accuracy, but this is not enough.
To get a better understanding for the method we consider the
behavior for a simple problem.

We take the linear, positivity preserving ODE
\cite{kopecz_unconditionally_2018}
\begin{equation}
  u'(t) = L u(t),
  \quad
  u(0) = \begin{pmatrix} 1 \\ 0 \end{pmatrix},
  \qquad
  L = \begin{pmatrix} -5 & 1 \\ 5 & -1\end{pmatrix},
\end{equation}
and use the three stage, third order SSP method SSP(3,3)
\begin{align}
\begin{array}{c|ccc}
0 &  &  & \\
1 & 1 &  & \\
\nicefrac{1}{2} & \nicefrac{1}{4} & \nicefrac{1}{4} & \\
\hline
 & \nicefrac{1}{6} & \nicefrac{1}{6} & \nicefrac{2}{3}\\
\end{array}
\end{align}
of \cite{shu1988efficient}.
The matrix $L$ has the eigenvalues zero and $-6$ and its operator
norm is $2 \sqrt{13}$.
The real stability interval of SSP33 includes $[-2.5,0]$. Hence,
it seems reasonable\footnote{A more careful analysis following
\cite{ranocha2018L2stability} shows that the method is not
monotone (energy stable) for this problem and choice of $\dt$ since
$L$ is not normal.} to choose $\dt = \nicefrac{1}{3}$.
The corresponding stage derivatives are
\begin{equation}
  f(y_1) = \begin{pmatrix} -5 \\ 5 \end{pmatrix},\quad
  f(y_2) = \begin{pmatrix} 5 \\ -5 \end{pmatrix},\quad
  f(y_3) = \begin{pmatrix} -5 \\ 5 \end{pmatrix}.
\end{equation}
The values of the next step using the standard weights is
\begin{equation}
  u^1 = \begin{pmatrix} \nicefrac{-1}{9} \\ \nicefrac{10}{9} \end{pmatrix}.
\end{equation}
Since the first component of the new solution is negative,
we want to adapt the weights to ensure positivity.
All weights that comply with the constraints for first and second
order of accuracy can be expressed as
\begin{equation}
  \bt =
  \begin{pmatrix}
    \nicefrac{1}{6} \\
    \nicefrac{1}{6} \\
    \nicefrac{2}{3}
  \end{pmatrix}
  + \alpha \begin{pmatrix}
    \nicefrac{1}{2} \\
    \nicefrac{1}{2} \\
    -1
  \end{pmatrix},
  \qquad
  \alpha \in \R.
\end{equation}
We have one degree of freedom for the choice of the weights,
parameterized by $\alpha$.
If the general expression for the weights is inserted in
\eqref{eq:Combination} the general solution is
\begin{equation}
  u^{1}
  =
  u^0 + \dt \left(f_1, f_2, f_3\right) \bt
  =
  \begin{pmatrix}
    \nicefrac{-1}{9} \\
    \nicefrac{10}{9}
  \end{pmatrix}
  +\alpha \begin{pmatrix}
    5 \\
    -5
  \end{pmatrix}.
\end{equation}
By changing the parameter $\alpha$, the weights and the new
solution are altered. With a suitable choice of
$\alpha \in \left[ \nicefrac{1}{45}, \nicefrac{2}{9} \right]$,
any $u$ that complies with mass conservation and positivity can
be reached.
By adding additional constraints on the weights, the choice of
$\alpha$ can be narrowed down.
An objective function is also needed to make the choice unique. This should be designed in a way to prefer weights that are close to the original weights.


\subsection{Order conditions}\label{sec:OrderCond}

The order conditions for an $s$-stage, order $p$ RKM are a set of equations depending on $A$, $b$ and
$c$.  As mentioned already, if $A$ and $c$ are given, the order conditions are
linear in $b$ and can be written as $Q_p b = r_p$, where
$Q_p\in{\mathbb R}^{v\times s}, r_p\in{\mathbb R}^v$ represent the set of all conditions up to
and including order $p$. Here $v$ is the number of order conditions. It may not be possible to find modified weights
that also satisfy the conditions of order $p$ and yield positivity, so
in general the modified weights will be a solution of
$$
  Q_\pt \bt = r_\pt
$$
for some $\pt \le p$.   Since we have $s$ degrees of freedom $\bt_j$,  we need
at a minimum to choose $\pt$ so that $\mathrm{rank}(Q_\pt) < s$.
Because the quadrature conditions are linearly independent, we have $\mathrm{rank}(Q_p)\ge p$,
so we must take $\pt \le s$.  In general we may need to take $\pt$ even smaller
in order to achieve positivity.


\subsection{Choice of objective function and additional constraints}
In the design of Runge--Kutta methods, weights are carefully chosen
not only to satisfy the order conditions but also
to give desirable properties such as a good region of absolute
stability, small error coefficients, and so forth.
Replacing these carefully-chosen weights $b$ with arbitrary weights
$\bt$ could lead to the loss of these desirable properties.
In order to preserve good properties of the method, we use as
objective function $\|\bt - b\|_1$.  This has the additional benefit
of penalizing weights with large magnitude in general, avoiding
large truncation or cancellation errors.  This also ensures that if
no negative solution values appear, the solution of the LP is
simply the original method weights.  Thus we have the following LP:

\LPblock{
(Free adaptation) Given $F$, $\pt$, and $b$, find $\bt$ that minimizes $\|\bt - b\|_1$ subject to
\begin{subequations}
\label{eq:free-adaptation}
\begin{align}
u^{n+1}&=u^n+\dt F \bt \geq 0, \label{eq:direct_pos}\\
Q_\pt\bt &=r_\pt \label{eq:direct_Order}.
\end{align}
\end{subequations}
}

Of course, there is still no guarantee that the modified weights
will be close to the original method weights.  In some examples
we have observed that large modifications of the weights can lead
to inaccurate solutions even though the order conditions are satisfied.
In order to avoid issues that might be caused by poor weights, we can
additionally impose either or both of the following strategies:

\begin{enumerate}
    \item Select in advance a set of desirable weight vectors $b^1, b^2, \dots, b^m$
        corresponding to known good methods,
        and restrict the choice of $\bt$ to convex combinations of this set.
    \item Require that the perturbation $\|\tilde u - u\|$ is small and reject the step if it is not.
\end{enumerate}

We discuss the first strategy here; the second is deferred to section\,\ref{sec:error}.
Ideally every element of the set of potential weight vectors would correspond to
a method of the same order as the original method.   Due to linearity of the
order conditions, any linear combination of such weights would also yield a
method of the same order.  On the other hand,
it is natural to include a weight vector corresponding to the forward Euler method
(for explicit methods) or backward Euler method (for implicit methods), since
these two methods guarantee positivity (unconditionally for backward Euler
and conditionally for forward Euler).
We can formulate an LP using the first strategy (restriction to a convex set)
as follows.  Let $B$ denote the matrix with columns $b^1, b^2, \dots, b^m$ and let
$g\in{\mathbb R}^m$.  The LP is then as follows:

\LPblock{
(Convex adaptation) Given $F$, $B$, and $b$, find $g$ that minimizes $\|\bt - b\|_1$ subject to
\begin{subequations}
\begin{align}
\bt & = Bg, \\
0 & \le g_i \le 1, \\
\sum_i g_i &= 1, \\
u^{n+1}&=u^n+\dt F \bt \geq 0.
\end{align}
\end{subequations}
}

Note that we do not need to impose the order conditions here, since they will be
satisfied by each of the methods and thus (by linearity) by the modified
method.  The order of the modified method will in general be equal to the lowest
order among the component methods.

Both approaches are illustrated in Figure\,\ref{fig:b_space}.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \input{Image_b_direct}
        \caption{Free adaptation}
        \label{fig:b_direct}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \input{Image_b_convex}
        \caption{Convex adaptation}
        \label{fig:b_convex}
    \end{subfigure}
    \caption{Graphical representation of the two different approaches to adapt the weights for
    a two-stage method.}\label{fig:b_space}
\end{figure}



\section{Properties of adaptive RKMs} \label{sec:integration}

In the previous sections an algorithm for choosing positivity preserving weights $\bt$ has been presented.
In the next section properties of the adaptive RKMs are discussed.

\subsection{Choice of baseline method}
An important property of the baseline method is the existence of embedded methods and the degrees of freedom for the weights $\bt$.
As noted in Section~\ref{sec:OrderCond} the number of stages has to be higher than the order.
Therefore, explicit RKMs are interesting, because an explicit RKM of order $p$ requires at least $p$ stages.
Fully implicit RKMs can achieve higher orders.
To fulfill $p<s$ the order has to be reduced drastically. Therefore, these methods are not good candidates for the adaptation.
As stated in \cite{norsett_attainable_1977} the highest achievable order of a diagonally implicit RKM is $s+1$. This condition is far closer to the condition required for the adaptation.
Another important property is the number of degrees of freedom for the choice of the new weights.
These can be calculated using $s-\mathrm{rank}(Q_{\pt})$. %$\mathrm{dim}(\mathrm{ker}(Q))$.
The number of degrees of freedom for different explicit methods are shown in Table\,\ref{table:DOF_exp}.

\begin{table}[h!]
\centering    %Generated below============
% \begin{tabular}{|l c|c c c c c c |}
%  \hline
% Order & &1&2&3&4&5&6 \\
%  \hline Classical RK4& s=4&3&2&0&0& - & -  \\
%  SSPRK(10,4)& s=10&9&8&6&4& - & -  \\
%  Cash-Karp RK5(4)6& s=6&5&4&2&1&0& -  \\
%  Dormand-Prince RK5(4)7& s=7&6&5&3&1&0& -  \\
%  \hline
%  \end{tabular}
  \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lr*6c@{}}
    \toprule
    Method & $s$ & \multicolumn{6}{c}{Order $\tilde p$} \\
    & & 1 & 2 & 3 & 4 & 5 & 6 \\
    \midrule
    Classical RK4 \cite{kutta1901beitrag} & 4 & 3 & 2 & 0 & 0 & --- & --- \\
    SSPRK(10,4) \cite{ketcheson2008highly} & 10&9&8&6&4& --- & ---\\
    Cash--Karp RK5(4)6 \cite{cash1990variable} & 6&5&4&2&1&0& --- \\
    Dormand--Prince RK5(4)7 \cite{prince1981high}& 7&6&5&3&1&0& --- \\
    \bottomrule
  \end{tabular*}
  \caption{Degrees of freedom for the choice of the weights for some explicit methods.} %Generated above============
  \label{table:DOF_exp}
\end{table}

\begin{table}[h!]
\centering   %Generated below============
%   \begin{tabular}{|l c|c c c c c c |}
%   \hline
%   Order & &1&2&3&4&5&6 \\
%   \hline Implicit Euler& s=1&0& - & - & - & - & -  \\
%   Lobatto~IIIC4& s=4&3&2&1&0&0&0 \\
%   Radau~IIA3& s=3&2&1&0&0&0& -  \\
%   SDIRK(5,4)& s=5&4&3&1&0& - & -  \\
%   TR-BDF2& s=3&2&1& - & - & - & -  \\
%   Im-Euler 2& s=3&2&1& - & - & - & -  \\
%   Im-Euler~3& s=6&5&4&2& - & - & -  \\
%   Im-Euler 4& s=10&9&8&6&3& - & -  \\
%   \hline
%   \end{tabular}
   \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lr*6c@{}}
    \toprule
    Method & $s$ & \multicolumn{6}{c}{Order $\tilde p$} \\
    & & 1 & 2 & 3 & 4 & 5 & 6 \\
    \midrule
    Implicit Euler& 1&0& --- & --- & --- & --- & ---  \\
    Lobatto~IIIC4 \cite{chipman1971stable} & 4&3&2&1&0&0&0 \\
    Radau~IIA3 \cite{ehle1969pade} & 3&2&1&0&0&0& ---  \\
    SDIRK(5,4) \cite[eq. (6.18)]{hairer_solving_1996}& 5&4&3&1&0& --- & ---  \\
    TR-BDF2 \cite{bank1985transient} & 3&2&1& --- & --- & --- & ---  \\
    Extrapolation Im-Euler 2& 3&2&1& --- & --- & --- & ---  \\
    Extrapolation Im-Euler~3& 6&5&4&2& --- & --- & ---  \\
    Extrapolation Im-Euler 4& 10&9&8&6&3& --- & ---  \\
    \bottomrule
  \end{tabular*}
  \caption{Degrees of freedom for the choice of the weights for some implicit methods.} %Generated above============
  \label{table:DOF_imp}
\end{table}


For all methods with the number of stages equal to the order of the method, there are no degrees of freedom without reducing the order.
If the classical RK4 method is used the order has to be reduced more because the RK4 method does not have embedded methods of order~3.
In contrast to this, some methods with $s > p$ have degrees of freedom even without reducing the order.
An example of this is SSPRK(10,4), that has 4 degrees of freedom for $\pt = p$.
For Cash--Karp RK5 and Dormand--Prince RK5 the order has to be reduced to get degrees of freedom even though the number of stages is higher than the order.


In Table\,\ref{table:DOF_imp}, the same properties of some implicit
methods are reported.
We can see that the fully implicit methods Lobatto~IIIC4 and Radau~IIA3 require a drastic reduction of the order as expected.
The diagonally implicit SDIRK(5,4) method only requires an order reduction of one to get one degree of freedom for the weights.
The TR-BDF2 method even allows adaptations without reducing the order.
The implicit Euler extrapolation methods also exhibit degrees of freedom without a reduction of the order.

Another important property is the stability region.
The adaption of the weights does not solve issues with stability when used with a too big $\dt$. Therefore, the RK method has to be stable for large $\dt$.

A third property is the existence of a positive solution. This corresponds to the question whether there is an embedded first order method that ensures positivity. This is particularly interesting for implicit methods, because $\dt$ can be unrestricted by stability requirements.
If we have an embedded first order method, we can ensure that there is always a positive solution, which might be of first order.
This is the case for the Im-Euler extrapolation methods, because they contain a chain of BE steps.
For explicit methods the FE method is always embedded, but a reduction of the step size might be required.

\subsection{Error detection and approximation}\label{sec:error}
For this approach it is difficult to give some mathematically rigorous statement regarding the convergence of the method, because the RKM is only adapted if the step sizes are large.
For sufficiently small step sizes the solution of the unadapted RKM is positive. Therefore the original weights are used.
We have to describe the behavior outside the asymptotic regime.
%The main way to ensure that the RKM with adapted b converges to the correct solution is to ensure that the weights approaches $b_{orig}$ for $\dt \to 0$. For this the weights it is already known that the numeric solution converges to the exact solution.
%This is made sure by the property of the Objective function that
%\begin{equation}
%\mathrm{argmin}(f_{optim}(b)) = b_{orig}
%\end{equation}
To approximate the error of a new step we propose the following approximation of the local error:

\begin{align}
err = \|u(t^{n+1})-\tilde u^{n+1}\| &= \|u(t^{n+1}) - (u^{n+1}+\dt F(\bt-b))\| \\
 &\leq \underbrace{\|u(t^{n+1})-u^{n+1}\|}_{\approx err_T}+\underbrace{\|\dt F(\bt-b)\|}_{= perturb} \label{eq:Err}
\end{align}

The total error is split up in the truncation error $err_T$ and the perturbation $perturb$ using the triangle inequality.
The truncation error can be estimated using the standard error estimators $err_T = \| u^{n}_{b} - u^{n}_{\hat{b}} \|$.
After adapting the weights, the perturbation is calculated. If the perturbation is larger than the tolerance the weights are rejected.
Both values are added to get an approximation of the total error $err = err_T + perturb$.
% The truncation error and the adaption error never have the same direction.
% Therefore, the triangle inequality becomes a strict inequality and the real error is always smaller than the estimate. (Note: as long as the Truncation error estimator is right)
%\subsubsection{Steppsize Control}
%An important part of an RKM method is the ability to approximate the error of the solution to use it to control the step size.
%The truncation error $|u(t^n)-u^n_{b_{orig}}|$ is usually approximated with an standard error estimator $err_T = | u^{n}_{b_{orig}} - u^{n}_{\hat{b}} |$.
 %The parameter $w_a \leq 1$ is used to account for the fact that the real error is smaller than $err = err_T +err_{adapt}$. The factor also improves the stability of the step size control because $err_{adapt}$ is not as smooth as $err_T$, especially if negative values only occur for a small number of steps.
%If the LP was infeasible and no weights were found the $err_{approx}$ is set to a custom value. This value should be chosen sufficiently large to ensure that the next step take is smaller, but still not too large to keep the step size control stable.
This type of error estimation is easy to implement because it can be easily incorporated in an existing step size control and takes advantage of the standard error approximation.

%To make sure that the new adapted solution is still reasonable we measure the deviation from the Original method. In the worst case the errors add up. (todo: here some math formula... (We know that cannot be the case because we already know that we will reduce the error for the quantity we will make positive, still we can use it as a upper bound.))

\subsection{Stability region}

Adapting the weights $b$ changes the RK method. Hence, the stability
function is altered and the region of absolute stability varies.

As an example, the stability regions of adapted RKMs are visualized in
Figure\,\ref{fig:stab}.
In Figure\,\ref{fig:stab_dp5} the Dormand--Prince RK5 method is freely adapted.
The weights are taken from the example in Section~\ref{sec:Ex_expl}.
In Figure\,\ref{fig:stab_ex3} the stability regions of the Im-Euler~3
extrapolation method and the embedded chain of three BE steps are
plotted. Additionally the stability regions of convex combinations
of these two methods are shown.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/stab_dp5.pdf}
         \caption{Dormand--Prince RK5 with free adaptation
                   of the weights $\bt$ as in the example in Figure~\ref{fig:weights_AdDe}.}
         \label{fig:stab_dp5}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/stab_ex3.pdf}
         \caption{Im-Euler~3 extrapolation method and embedded chain
                  of three BE steps with convex combinations of both
                  weights.}
         \label{fig:stab_ex3}
     \end{subfigure}
        \caption{Change of stability region for the free adaptation and
                 the convex adaptation of the weights.
                 Regions with $|R(z)| \leq 1$ are hatched for the original methods.}
        \label{fig:stab}
\end{figure}


The stability function is defined as a function $\CN \to \CN$ with
\begin{equation}
R(z) = 1 + zb^T(I - zA)^{-1}e,
\end{equation}
where $e = (1, \dots, 1)^T \in \R^s$.
We can add $b$ as a second parameter and get a function
$R\colon \R^s \times \CN  \to \CN$ with
\begin{equation}
R_b(z) = 1 + zb^T(I - zA)^{-1}e.
\end{equation}
The stability function is an affine-linear function of the weights.

%TODO: It is not straightforward if there are no pole zero cancellations in the stability function. For explicit methods, the stability function is a polynomial o degree $\leq s$ (Hundsdorfer). Pole zero cancellations cannot happen for these. For implicit functions that are A-stable the poles have to be in the right halve plane. Here is starts to get interesting. Because the A-Stability of one method with a certain $A$ is not a sufficient condition that there are no Poles for all RKM with this A-Matrix. But it is possible (or easy) to prove that if $a_{ii} > 0 \forall i = 1,\cdots , s$ then there are only poles in the right halve plane where they do not matter for the further considerations.)


\subsubsection{Stability of convex adaptation}

If the new weights are chosen by convex adaptation of given weights,
it is easy to prove some properties of the stability region.
\begin{theorem}
  The stability region of a Runge--Kutta method $(A,\bt)$ where
  $\bt = \sum_{i} g_i b^i$ is a convex combination of
  $b^1, \dots, b^m \in \R^s$ (i.e.\ $g_i \in [0,1]$, $\sum_i g_i = 1$),
  contains at least the intersection of the stability regions of
  the methods $(A,b^1), \dots, (A,b^m)$.
\end{theorem}
\begin{proof}
  Since the stability function is an affine-linear function of the
  weights, $R_{\sum_i g_i b^i}(z) = \sum_i g_i R_{b^i}(z)$. Hence,
  if $z$ is in the stability region of all methods $(A,b^1), \dots, (A,b^m)$,
  \begin{equation}
    | R_{\sum_i g_i b^i}(z) |
    \le
    \sum_i g_i | R_{b^i}(z) |
    \le
    1.
  \end{equation}
\end{proof}

This result is particularly important for implicit methods.
If all the embedded methods used to construct the new weights
are A-stable, the resulting method is also A-stable.


\subsubsection{Stability of free adaptation}
If the weights is adapted freely there is no such proof.
A useful property of the resulting stability function is that it is similar to the stability function of the baseline method.

\begin{lemma}
  The stability function $R_{\bt}$ of an adapted RK method
  satisfies
  \begin{equation}
    | R_{\bt}(z) |
    \le
    | R_{b}(z) |
    + \| \bt - b \|_1 \| z (\I - z A)^{-1} e \|_\infty.
  \end{equation}
\end{lemma}
\begin{proof}
  Compute
  \begin{equation}
  \begin{aligned}
    | R_{\bt}(z) |
    &\le
    | R_{b}(z) |
    + | R_{\bt}(z) - R_{b}(z) |
    =
    | R_{b}(z) |
    + | z (\bt - b) (\I - z A)^{-1} e|
    \\
    &\le
    | R_{b}(z) |
    + \| \bt - b \|_1 \| z (\I - z A)^{-1} e \|_\infty.
  \end{aligned}
  \end{equation}
\end{proof}
This shows why the objective $\min \| \bt - b \|_1 $ is a good
choice to control the change of the region of absolute stability,
in particular for explicit methods for which
$\| z (\I - z A)^{-1} e \|_\infty$ can be bounded by a polynomial
in $|z|$.


\section{Implementation aspects of the algorithm}\label{sec:imple}

Having described the mathematical idea of our approach,
we discuss some practical implementation details of the
proposed algorithm in the following.


\subsection{Simplification of positivity constraints}
The number of positivity constraints implied by \eqref{eq:ut}
is equal to $m$, the number of ODEs being solved.  This number
may be very large, for instance if the system is a semi-discretization
of a PDE.  This makes solution of the LP very costly.
But in most cases, positivity is violated only for a very small subset $h \subseteq \{1, \dots, m\}$ of
the solution components.  We can solve a much less expensive LP
by replacing \eqref{eq:ut} with
\begin{equation}
u_i^n + \dt \sum_{j=0}^s F_{i,j}  b_j  \geq 0 \quad \forall i \in h \subseteq \{1,\dots,m \}.
\end{equation}
Of course, it must be checked that the solution of the resulting LP still
satisfies the full set of constraints \eqref{eq:ut}.  In practice, we
have found the following approach to be effective.
First, set
$$h_0 = \{ i \in \{1,\dots,m \} \ | \  u_i^{n+1}  < 0 \}.$$
Solve the LP and let $\ut^{n+1}$ denote the new solution.
If $\ut^{n+1}$ satisfies \eqref{eq:ut}, accept this as the new
solution; otherwise, repeatedly take
$$h_{a+1} = \{ i \in \{1,\dots,m \}|  \ut_i^{n+1}  < 0 \} \cup h_{a}$$
until $\ut^{n+1}$ is found to satisfy \eqref{eq:ut}.
In the examples we have studied, this approach was found to always converge in
at most X iterations.
%(Note: A approach on this would be to make a educated guess which indices could get a problem with positivity. A possible estimate would be $\frac{u_i^{n+1}}{max(K_{(i,0)}, \cdots ,K_{(i,0)})} $ )

When enforcing a maximum value, the number of constraints can be reduced using the same technique. When enforcing both maximum and minimum values two separate sets of active constraints are used.
In this case it is important to update these sets simultaneously.

\subsection{Implementation}

In this section an algorithm to calculate the solution of an ODE using the adaptive RKM is proposed.
The pseudocode is given in Algorithm\,\ref{alg:Adaption}.

To calculate the next solution $u^{n+1}$ the following is algorithm is executed. This is repeated until the final time $t_{end}$ is reached.

At first, a step size $\dt$ is selected using a step size control algorithm or a fixed step size is used.
With this $\dt$ the stage values $y_i$ and stage derivatives $f_i$ are calculated.
Afterwards, $u^{n+1}$ is computed using the original weights $b$.
If the positivity is not violated, this solution is used to update the solution.
If $u^{n+1}$ contains negative values, the weights are adapted using the following algorithm.

Start with $\pt$ equal to the highest order $p_{start}$ such that there are degrees of freedom for the weights $\bt$.
Apply an LP solver to the optimization
problem.
If the LP is infeasible, reduce the order $\tilde p$ by one.
If the LP is feasible, test whether the solution is acceptable.
At first, test whether the new $u^{n+1}$ satisfies the constraints.
This is necessary because sometimes the applied LP solvers incorrectly
identify the problem as feasible.
Afterwards the perturbation is calculated using \eqref{eq:Err}.
If the perturbation is bigger than a given tolerance, the
new weights $\bt$ are also rejected.
If the new set of weights is rejected in one of the tests the order
$\tilde p$ is reduced by one.
As soon as the algorithm finds suitable weights, these are used to update the solution.
If all acceptable orders are tried and no weights were found, the step hast to be rejected.

\begin{algorithm}[ht]
\begin{algorithmic}[1]
\While{$t<t_{end}$}
\State Choose $\Delta t$
\State Calculate $F = (f_1, \dots, f_s)$ \eqref{eq:stagevalues}
\State Set $u^{n+1}=u^n + \Delta t Fb$ \eqref{eq:Combination}
\If {$u^{n+1} \geq 0$}
	\State $success = True$
\Else
	\State{$\tilde p = p_{start}$}
	\While{$\tilde p \geq p_{min}$}
		\State Solve LP \eqref{eq:free-adaptation}
		\If {LP Feasible \textbf{and} result correct}
			\State $perturb = \| \Delta t F(\bt-b)\|$
			\If {$perturb < tol$}
				\State $u^{n+1}=u^n+\Delta t F \tilde b$
				\State $success = True$
				\State \textbf{break}
			\EndIf
		\EndIf
		\If {$\tilde p = p_{min}$}
			\State $sucess=False$
		\EndIf
		\State $\tilde p = \tilde p -1$
	\EndWhile
\EndIf
\If {$sucess$}
% 	\State $u^n=u^{n+1},t^n=t^{n+1}+\Delta t,n=n+1$
	\State Set $n=n+1$ and continue
\EndIf
\EndWhile
\end{algorithmic}
\caption{Pseudocode for the algorithm using a free adaption of weights.}
\label{alg:Adaption}
\end{algorithm}

\section{Results of numerical experiments}\label{sec:Numeric_Results}

The implementation of the algorithms described above and code to
reproduce the numerical examples reported here can be found in
\cite{repository}. \todo{TODO: New repository with DOI} %TODO
The methods are implemented in Python using NumPy/SciPy
\cite{virtanen2020scipy}, NodePy \cite{nodepy08}, and
Matplotlib \cite{hunter2007matplotlib} for the visualizations.
We have used MOSEK \cite{mosek} via CVXPY to solve the LPs
\cite{cvxpy, cvxpy_rewriting}.

The adaptive RKM can be used with ODEs that satisfy \eqref{eq:condition_ODE_pos}.
For problems where the exact solution is positive for certain $u_0$ but do not satisfy \eqref{eq:condition_ODE_pos} tests did not show promising results. Additionally, it is not certain whether the computed solutions would be reasonable.


\subsection{Non-stiff problem with fixed stepsize}\label{sec:Ex_expl}
First, adaptive RKMs based on explicit methods are tested on non-stiff problems with a fixed step size.
When used with explicit methods the cost of solving the LP is significant because the computation of the stage derivatives only requires $s$ evaluations of the RHS.
For most of the linear test problems tried, the explicit methods yield to positive results.
When increasing the step size, issues with stability occur before getting negative values.
An example for this is the ODE in Section~\ref{sec:example_lin}.
Some nonlinear RHS may require very small time steps to preserve positivity.
For these, adapting the weights could be a possible way to solve them.
An example is the reaction equation solved in Section~\ref{sec:example_reac}.
Since the positivity of the stage values is not ensured, the right hand side has to be defined for negative values. If this is not the case, the right hand side has to be adapted accordingly.

A test problem similar to \cite{shampine_non-negative_2005} is the PDE
\begin{equation}
\label{eq:advection-decay}
\begin{aligned}
  \frac{\partial u(t,x)}{\partial t}
  &=
  -a \frac{\partial u(t,x)}{\partial x} - K u(t,x),
  && x \in (0, 1), t \in (0,1),
  \\
  u(t,0) &= 1,
  && t \in (0,1],
  \\
  u(0,x) &= 0,
  && x \in [0,1],
\end{aligned}
\end{equation}
which consists of an advection part and an exponential decay.
The numerical approximation uses the method of lines and a first order
upwind finite difference semidiscretization with $N = 100$ points.
This leads to the positivity preserving ODE
\begin{equation}
\frac{\mathrm d}{\mathrm d t} u_i = \frac{a}{\Delta x} \left( u_{i-1} - u_i \right) - K u_{i-1}.
\end{equation}
The parameters are set to $a=1$ and $K=1$.
We use the Dormand--Prince RK5 method and adapt the weights using the free adaptation.
In Figure\,\ref{fig:sol_AdDe} the results for $\Delta t = 0.02$ are plotted for different time steps. We can see that the solution approaches an exponential function with $t \rightarrow \infty$.
In Figure\,\ref{fig:weights_AdDe} the used weights are plotted. For $t\leq 0.6$ the weights are altered. For $t > 0.6$ the original weights lead to a positive solution.
\begin{figure}
\centering
\begin{subfigure}[t]{0.45\textwidth}
\centering
\includegraphics[width=1\textwidth]{plots/Advection_Decay.pdf}
\caption{Numerical solution at different times.}
\label{fig:sol_AdDe}
\end{subfigure}%
~
\begin{subfigure}[t]{0.45\textwidth}
\centering
\includegraphics[width=1\textwidth]{plots/b_Advection_Decay.pdf}\\
\caption{Adaptation of the weights.}
\label{fig:weights_AdDe}
\end{subfigure}
\caption{Numerical results for the advection decay problem \eqref{eq:advection-decay}.}
\end{figure}


Next, different time steps are used.
For $\dt \leq 0.0082$ the unaltered method leads to positive solutions. For a larger $\dt$ the original method leads to negative values and the weights are altered. For $\dt >0.016$ the baseline method is no longer stable.
For the ODE, the reference solution can be computed using the matrix exponential.
In Figure\,\ref{fig:conv_expl} the convergence for $t=0.5$ is plotted for the altered and unaltered method.

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{plots/conv_adde.pdf}
\caption{Convergence results of the adapted Dormand--Prince RK5 for the advection decay problem \eqref{eq:advection-decay}.}
\label{fig:conv_expl}
\end{figure}

The unaltered method has the order $p=5$.
The values marked with a cross denote the numerical experiments that required an adaption of the weights.
Even though the order is reduced, most errors are still close to the error of the unaltered method.

\subsection{Stiff problem with fixed step size}
Next, adaptive RKMs based on implicit methods are tested on stiff problems.
Implicit methods are an advantageous choice for a couple of reasons.
Firstly, the cost of solving the LP is relatively small compared to the cost of solving the stage equations.
Secondly, the time step is not limited by the stability of the method.
Therefore, it is possible to use larger time steps that are more likely to lead to negative values.

A very interesting class of methods are the implicit extrapolation methods. These are diagonally implicit and $p < s$. This means that they allow for changes of the weights without a reduction of the order.
Moreover, all stage values are computed using the BE method. Hence, all intermediate stages are positive.
Furthermore, an embedded BE step is included. This ensures that a positive solution always exists, even if it is of first order.

We test the proposed adaptation algorithm on the diffusion equation
\begin{equation}
\label{eq:diffusion}
\frac{\partial }{\partial t} u = D \frac{\partial^2}{\partial x^2} u
\end{equation}
with homogeneous Dirichlet boundary conditions on the domain $x = [-0.5,0.5]$ with $N=100$ points. The equation is semidiscretized using the 3-point-scheme
\begin{equation}
\frac{\mathrm d}{\mathrm d t} u_i = \frac{d}{\Delta x^2} \left( u_{i-1} - 2u_i + u_{i+1} \right).
\end{equation}
As initial condition $u^0 = (0,\cdots,0,1,0,\cdots,0)^T$ is used. The diffusion coefficient is $D=1$.

The ODE is solved using the Im-Euler~3 extrapolation method.
For large $\dt$ the method computes negative values for $u^1$.
These can be corrected by adapting the weights.
The solutions are computed using the free adaptation and convex adaptation for $\dt = \num{1e-3}$.
The results for the free adaptation are plotted in Figure\,\ref{fig:sol_Diff_a} and the corresponding change of the weights
is shown in Figure\,\ref{fig:weights_Diff_a}.
The original solution for the first step is negative. Therefore, the weights have to be changed.
If we take a look at the solution after the first time step at $t=0.001$ we can see that at $x=0$ the solution is smaller than the solution at the surrounding points.
This is not physical.
The next time steps lead to physical solutions again.
To prevent this glitch from happening we choose the weights based on a convex adaptation.
A first order embedded method is added. The solution is shown in Figure\,\ref{fig:sol_Diff_c} and the weights are visualized in Figure\,\ref{fig:weights_Diff_c}.
The weights for the first step are altered again.
The weights obtained by the convex adaptation are different from the weights obtained
by taking the free adaptation.
The solution for $t=0.001$ computed with the convex adaptation is physical.
For both approaches, the remaining steps can be computed with the standard weights.
\begin{figure}
\centering
\begin{subfigure}[b]{0.45\textwidth}
\centering
\includegraphics[width=1\textwidth]{plots/Diff_Direct.pdf}
\caption{Solution using the free adaptation.}
\label{fig:sol_Diff_a}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\centering
\includegraphics[width=1\textwidth]{plots/Diff_Convex.pdf}
\caption{Solution using the convex adaptation.}
\label{fig:sol_Diff_c}
\end{subfigure}
\\
\begin{subfigure}[b]{0.45\textwidth}
\centering
\includegraphics[width=1\textwidth]{plots/b_Diff_Direct.pdf}
\caption{Change of weights for free adaptation.}
\label{fig:weights_Diff_a}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\centering
\includegraphics[width=1\textwidth]{plots/b_Diff_Convex.pdf}
\caption{Change of weights for convex adaptation.}
\label{fig:weights_Diff_c}
\end{subfigure}
\caption{Numerical results for the diffusion problem \eqref{eq:diffusion} and the
         adapted Im-Euler~3 extrapolation method.}
\end{figure}


In Figure\,\ref{fig:conv_impl}, the convergence is shown for the unaltered Im-Euler~3 extrapolation method (ex3, potentially resulting in negative values), the adaptive method with free adaptation (ex3a) and the adapted method using convex adaptations (ex3c).
Also, the BE method is plotted. It is only of 1st order but preserves positivity for all $\dt$.
For $\dt < \num{3e-5} $ the standard weights yield to a positive result. For larger $\dt$ the weights have to be adapted to ensure positivity.
The ex3a shows similar convergence properties as the original method.
This can be expected, because the adapted method is still of 3rd order.
The ex3c has larger errors than the ex3a but leads to physical solutions for all time-steps.
This is no surprise because the adapted RKM used for the first step is only of first order.
But the adaptive method still outperforms the BE, even when accounting for the higher cost per step.

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{plots/conv_heat.pdf}
\caption{Convergence result of baseline and adapted Im-Euler~3
         extrapolation methods for the diffusion problem \eqref{eq:diffusion}.}
\label{fig:conv_impl}
\end{figure}


\subsection{Stiff problem with adaptive step size}

Next we test the adaptive RKM on a more complex problem.
For this, we consider the advection-diffusion-production-destruction system  \cite{kopecz_comparison_2019}
\begin{subequations}
\label{eq:ADR}
\begin{align}
\frac{\partial u_1}{\partial t} &=-a \frac{\partial u_1}{\partial x} + d\frac{\partial^2 u_1}{\partial x ^2} + 0.01u_2 + 0.01 u_3 +0.003u_4 - \frac{u_1 u_2}{0.01+u_1}, \\
\frac{\partial u_2}{\partial t} &=-a \frac{\partial u_2}{\partial x} + d\frac{\partial^2 u_2}{\partial x ^2} + \frac{u_1u_2}{0.01+u_1} -0.01 u_2-0.5(1-\exp(-1.21 u_2^2)) u_3 -0.05 u_2, \\
\frac{\partial u_3}{\partial t} &=-a \frac{\partial u_3}{\partial x} + d\frac{\partial^2 u_3}{\partial x ^2} + 0.5(1-\exp(-1.21u_2^2)) u_3 - 0.01 u_3 -0.02 u_3, \\
\frac{\partial u_4}{\partial t} &=-a \frac{\partial u_4}{\partial x} + d\frac{\partial^2 u_4}{\partial x ^2} + 0.05 u_2 + 0.02 u_3 - 0.003u_4,
\end{align}
\end{subequations}
with parameters $a=\num{1e-2} $ and $ d=\num{1e-6}$.
The PDE is simulated on the domain $x = [0,1]$ with $N=100$ points and periodic boundary conditions.
The advection part is semidiscretized using a first order upwind scheme and the diffusion part is semidiscretized using a central 3-point-scheme. This leads to a positivity preserving system of ODEs which conserves the total mass $\sum u$.
The computation is done using the Im-Euler~3 extrapolation
method with free adaptation.
As step size control a PI-control from\,\cite{hairer_solving_1996} is used. The error was estimated using \eqref{eq:Err}. The tolerance was set to $Tol = 0.01$.
The final time is $t_{end} = 50$.

The simulation required 264 steps. Of these, 72 required an adaptation of the weights.
All adapted weights are still of 3rd order.
The solutions for $t=9,t=18,t=27$ and $t=50$ are plotted in Figure\,\ref{fig:Sol_ADP}.
In Figure\,\ref{fig:sol_ADP18} it can be seen that the reaction occurs in a small interfaces.
Outside of this regions quantities are close to zero. Therefore, it is very likely that negative values occur in the numerical approximation.
In Figure\,\ref{fig:sol_ADP27} it can be seen that at $T=27$ the two reaction interfaces merged. Afterwards the reaction stops and the behavior is mainly controlled by the advection and diffusion part.

In Figure\,\ref{fig:Stats_ADP} different values are plotted.
In the first subplot the step size is plotted. For $t<25$ the time steps are small. After $t = 30$ the step size increases, because the solution only evolves slowly afterwards.
In the second subplot the minimum of $u_1,u_2,u_3,u_4$ is plotted for all time steps that initially lead to negative values. This value is computed before and after adapting the weights.
We can see that relatively large negative values occurred at some time steps.
After the adaption of the weights, all values are close to $0$. Therefore, the adaption of weights successfully preserved positivity.
In the third subplot the approximated truncation error $err_T$ and the perturbation $\perturb$ are plotted.
We can see that $\perturb$ is of a similar magnitude as the truncation error. Therefore, the total error of the method is not increased drastically.
In the next subplots objective function is plotted.
We can see that the changes to the weights are only very small. The adapted RKM is still very close to the original RKM.
In the last subplot the deviation of the sum over $u_1,u_2,u_3,u_4$ from the initial sum is plotted.
The mass is conserved within roundoff error.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{plots/ADP_sol_00.pdf}
        \caption{$t=0$.}
        \label{fig:sol_ADP00}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{plots/ADP_sol_18.pdf}
        \caption{$t=18$.}
        \label{fig:sol_ADP18}
    \end{subfigure}

    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{plots/ADP_sol_27.pdf}
        \caption{$t=27$.}
        \label{fig:sol_ADP27}
    \end{subfigure}
	\begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{plots/ADP_sol_50.pdf}
        \caption{$t=50$.}
        \label{fig:sol_ADP50}
    \end{subfigure}
    \caption{Solution of advection-diffusion-reaction problem \eqref{eq:ADR} at different times.}\label{fig:Sol_ADP}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.85\textwidth]{plots/ADP_stepsize,b.pdf}
\caption{Statistics of computation of the advection-diffusion-reaction problem \eqref{eq:ADR}.}
\label{fig:Stats_ADP}
\end{figure}


\subsection{Stiff problem with adaptive step size II}

Here, we consider the stratospheric reaction problem of
\cite{sandu2001positive}, which models the reactions of the substances
in the concentration vector $u = [O^{1D}, O, O_3, O_2, NO, NO_2]$.
This ODE has two linear invariants
\begin{align}
  m_O^T u &= \text{const}, & m_O &= [1,1,3,2,1,2]^T, \\
  m_N^T u &= \text{const}, & m_N &= [0,0,0,0,1,1]^T,
\end{align}
which describe the conservation of the total mass of oxygen and nitrogen,
respectively.

The reaction system
\begin{equation}
\label{eq:stratospheric}
\begin{aligned}
  \frac{\mathrm d}{\mathrm d t} O^{1D} &= r_5 - r_6 - r_7, \\
  \frac{\mathrm d}{\mathrm d t} O      &= 2 r_1 - r_2 + r_3 - r_4 + r_6 - r_9 + r_{10} - r_{11}, \\
  \frac{\mathrm d}{\mathrm d t} O_3    &= r_2 - r_3 - r_4 - r_5 - r_7 - r_8, \\
  \frac{\mathrm d}{\mathrm d t} O_2    &= -r_1 - r_2 + r_3 + 2 r_4 + r_5 + 2r_7 + r_8 + r_9, \\
  \frac{\mathrm d}{\mathrm d t} NO     &= -r_8 + r_9 + r_{10} - r_{11}, \\
  \frac{\mathrm d}{\mathrm d t} NO_2   &= r_8 - r_9 - r_{10} + r_{11},
\end{aligned}
\end{equation}
with time $t$ in seconds is given by the reaction rates
\begin{equation}
\begin{aligned}
  r_{1}  &= k_1\, O_2,          &  k_{1}  &= \num{2.643e-10} \sigma^3, &
  r_{2}  &= k_2\, O\, O_2,      &  k_{2}  &= \num{8.018e-17}, \\
  r_{3}  &= k_3\, O_3,          &  k_{3}  &= \num{6.120e-4} \sigma, &
  r_{4}  &= k_4\, O_3\, O,      &  k_{4}  &= \num{1.567e-15}, \\
  r_{5}  &= k_5\, O_3,          &  k_{5}  &= \num{1.070e-3} \sigma^2, &
  r_{6}  &= k_6\, M\, O^{1D},   &  k_{6}  &= \num{7.110e-11}, \\
  r_{7}  &= k_7\, O^{1D}\, O_3, &  k_{7}  &= \num{1.200e-10}, &
  r_{8}  &= k_8\, O_3\, NO,     &  k_{8}  &= \num{6.062e-15}, \\
  r_{9}  &= k_9\, NO_2\, O,     &  k_{9}  &= \num{1.069e-11}, &
  r_{10} &= k_{10}\, NO_2,      &  k_{10} &= \num{1.289e-2} \sigma, \\
  r_{11} &= k_{11}\, NO\, O,    &  k_{11} &= \num{1.0e-8},
\end{aligned}
\end{equation}
where $M  = \num{8.120e16}$ and
\begin{align}
    T &= (t/3600) \bmod 24,
    \quad T_r = 4.5,
    \quad T_s = 19.5 \\
    \sigma(T) &=
    \begin{cases}
    0.5+0.5 \cos\Bigl(\pi \Bigl|\frac{(2T-T_r-T_s)}{(T_s-T_r)}\Bigr| \frac{(2T-T_r-T_s)}{(T_s-T_r)} \Bigr)       & \quad \text{if } T_r \leq T \leq T_s,\\
    0  & \quad \text{otherwise}.
    \end{cases}
\end{align}

The initial conditions are
\begin{equation}
  u(t_0) = [
    \num{9.906e+1},
    \num{6.624e08},
    \num{5.326e11},
    \num{1.697e16},
    \num{4.000e6},
    \num{1.093e9}
  ]^T.
\end{equation}
The system was normalized internally such that $\forall n\colon u_n(t_0) = 1$
for the computation to achieve a suitable error estimation.
The system is solved in the time from $t_{0} = \SI{12}{h}$ to $t_{end} = \SI{84}{h}$
using the Im-Euler~3 extrapolation method with free adaptation and step size
control.

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{plots/Stratospheric_time.pdf}
\caption{Numerical approximation of stratospheric reaction system
         \eqref{eq:stratospheric}.}
\label{fig:Stratospheric_time}
\end{figure}

The results are shown in Figure~\ref{fig:Stratospheric_time}.
The adapted solution is close to the reference solution obtained
with the unadapted Im-Euler~3 extrapolation method and a higher accuracy.
For this solution, 249 steps were computed; two of these were rejected due to
a violation of the error bound.
More details are shown in Figure~\ref{fig:Stats_Strat}. The rejected steps are drawn with thick crosses.
The step size $\dt$ undergoes multiple sudden changes due to the explicit dependence
on time of the problem.
The minimum values before and after the adaptation are also shown for all steps where the initial values were negative.
This was only the case for some time intervals. 25 steps exhibited negative values.
Almost all of them were very close to zero and the adaptation did only show a small improvement.
The smallest value of the solution is $\min(u) = \num{-1.59e-11}$.
The used weights are also very close to the original weights, except of the steps that were rejected anyway due to a violation of the tolerance.
The change of the two linear invariants are shwon in the last two subplots. Both are preserved within roundoff error.

\begin{figure}
\centering
\includegraphics[width=0.85\textwidth]{plots/Stratospheric_stepsize,b.pdf}
\caption{Statistics of computation of the stratospheric reaction system
         \eqref{eq:stratospheric}.}
\label{fig:Stats_Strat}
\end{figure}


\section{Conclusion} \label{sec:conclusion}

It is possible to adapt the weights to enforce positivity for RKMs that are not positivity preserving.
One main limitation is that the resulting order has to be lower than the number of stages.
An error approximation for this method was given.
The region of absolute stability is altered by changing the weights. This effect can be predicted or controlled.
Used with explicit methods the positivity for some test problems could be recovered.
Because the time step size is limited by the stability it is only useful for a small interval of time steps.
The adaptive method is mainly interesting for diagonally implicit methods.
The times step size is not limited by stability.
Also, the cost of solving the LP is not a crucial factor.
If the negative values occurring are not too large, which can be expected for most computations, adapting the weights is a potential way to ensure positivity.

%also if exactnes is not the limiting factor but positivity


\printbibliography

\end{document}
